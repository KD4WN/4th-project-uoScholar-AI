# chatbot.py - ëŒ€í™”í˜• ê³µì§€ ì¶”ì²œ ì±—ë´‡
import os
import re
import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
from collections import defaultdict
from math import sqrt
from dotenv import load_dotenv

load_dotenv()

from fastapi import FastAPI
from pydantic import BaseModel
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain.schema import Document
from langchain.embeddings.base import Embeddings
from pinecone import Pinecone
from sentence_transformers import SentenceTransformer
import cohere

# ===== í™˜ê²½ ì„¤ì • =====
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
CHAT_MODEL = os.getenv("CHAT_MODEL", "gpt-4o-mini")
EMBED_TYPE = os.getenv("EMBED_TYPE", "korean")
EMBED_MODEL = os.getenv("EMBED_MODEL", "jhgan/ko-sroberta-multitask")  # í•œêµ­ì–´ ëª¨ë¸
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX = os.getenv("PINECONE_INDEX", "uos-notices")
PINECONE_NS = os.getenv("PINECONE_NAMESPACE")
COHERE_API_KEY = os.getenv("COHERE_API_KEY")

# ëŒ€í™” ì„¤ì •
MAX_CONVERSATION_TURNS = None  # ë¬´ì œí•œ ëŒ€í™” í„´
TOP_K = int(os.getenv("TOP_K", "12"))

# ê²€ìƒ‰ ì„¤ì • (Cohere Reranker ì‚¬ìš©)
USE_RERANKER = os.getenv("USE_RERANKER", "true").lower() == "true"
INITIAL_SEARCH_K = int(os.getenv("INITIAL_SEARCH_K", "20"))  # ì´ˆê¸° ê²€ìƒ‰ ê°œìˆ˜ (ì†ë„ ìµœì í™”)
FINAL_TOP_K = int(os.getenv("FINAL_TOP_K", "5"))  # Reranker í›„ ìµœì¢… ê°œìˆ˜
RERANK_THRESHOLD = float(os.getenv("RERANK_THRESHOLD", "0.1"))  # Reranker ì ìˆ˜ ì„ê³„ê°’ (ë” ê´€ëŒ€í•˜ê²Œ)

# ===== ì „ì—­ ê°ì²´ =====
_llm = None
_embeddings = None
_vectorstore = None
_pc_client = None
_cohere_client = None


# ë¶ˆìš©ì–´ ì„¤ì •
STOPWORDS = {
    "ê³µì§€","ì•ˆë‚´","í”„ë¡œê·¸ë¨","ì›Œí¬ìˆ","í–‰ì‚¬","ê³µì§€ì‚¬í•­","ê³µì§€ìš”","ë¬¸ì˜","ì‹ ì²­",
    "ê´€ë ¨","ê´€ë ¨ëœ","ìˆì–´","ìˆë‚˜ìš”","í˜¹ì‹œ","ì¢€","ìš”","ê±°","ê²ƒ","ê°™ì•„","ì‹¶ì–´","ê²¨","ë¶€í„°","ê¹Œì§€"
}

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")

# ì„¸ì…˜ë³„ ëŒ€í™” ìƒíƒœ ê´€ë¦¬
conversation_state: Dict[str, Dict[str, Any]] = defaultdict(lambda: {
    "turns": 0,
    "messages": [],
    "user_requirements": {},
    "completed": False
})

def get_llm():
    global _llm
    if _llm is None:
        _llm = ChatOpenAI(
            model=CHAT_MODEL,
            temperature=0.1,
            api_key=OPENAI_API_KEY
        )
    return _llm

class KoreanSentenceTransformerEmbeddings(Embeddings):
    """í•œêµ­ì–´ ì„ë² ë”© í´ë˜ìŠ¤ (ìºì‹± ê¸°ëŠ¥ í¬í•¨)"""
    def __init__(self, model_name: str):
        self.model = SentenceTransformer(model_name)
        # ì§„í–‰ë¥  í‘œì‹œ ë¹„í™œì„±í™”ë¡œ ë¡œê·¸ ì •ë¦¬
        self.model.show_progress_bar = False
        # ì¿¼ë¦¬ ì„ë² ë”© ìºì‹œ
        self._query_cache = {}

    def embed_documents(self, texts):
        return self.model.encode(texts, convert_to_tensor=False, show_progress_bar=False).tolist()

    def embed_query(self, text):
        # ì¿¼ë¦¬ ì„ë² ë”© ìºì‹±ìœ¼ë¡œ ì†ë„ í–¥ìƒ
        if text in self._query_cache:
            return self._query_cache[text]

        embedding = self.model.encode([text], convert_to_tensor=False, show_progress_bar=False)[0].tolist()

        # ìºì‹œ í¬ê¸° ì œí•œ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
        if len(self._query_cache) > 50:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = next(iter(self._query_cache))
            del self._query_cache[oldest_key]

        self._query_cache[text] = embedding
        return embedding

def get_embeddings():
    global _embeddings
    if _embeddings is None:
        if EMBED_TYPE == "korean":
            _embeddings = KoreanSentenceTransformerEmbeddings(EMBED_MODEL)
            logging.info("âœ… Korean embedding model loaded: %s", EMBED_MODEL)
        else:
            _embeddings = OpenAIEmbeddings(
                model=EMBED_MODEL,
                api_key=OPENAI_API_KEY
            )
            logging.info("âœ… OpenAI embedding model loaded: %s", EMBED_MODEL)
    return _embeddings

def get_cohere_client():
    """Cohere í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    global _cohere_client
    if _cohere_client is None and COHERE_API_KEY:
        _cohere_client = cohere.Client(COHERE_API_KEY)
        logging.info("âœ… Cohere client initialized")
    return _cohere_client

def get_vectorstore():
    global _vectorstore, _pc_client
    if _vectorstore is None:
        if _pc_client is None:
            _pc_client = Pinecone(api_key=PINECONE_API_KEY)
            logging.info("âœ… Pinecone client initialized")

        _vectorstore = PineconeVectorStore.from_existing_index(
            index_name=PINECONE_INDEX,
            embedding=get_embeddings(),
            namespace=PINECONE_NS
        )
        logging.info("âœ… PineconeVectorStore ready: %s (ns=%s)", PINECONE_INDEX, PINECONE_NS)
    return _vectorstore


# ===== ëŒ€í™” ìš”êµ¬ì‚¬í•­ ë¶„ì„ =====
def extract_requirements(conversation_history: List[Dict[str, str]]) -> Dict[str, Any]:
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ì—ì„œ ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ"""

    # ëª¨ë“  ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ í•©ì³ì„œ ë¶„ì„
    user_messages = [msg["content"] for msg in conversation_history if msg["role"] == "user"]
    full_conversation = " ".join(user_messages)

    prompt = f"""ë‹¤ìŒ ëŒ€í™”ì—ì„œ ì‚¬ìš©ìê°€ ì°¾ê³  ìˆëŠ” ê³µì§€ì‚¬í•­ì˜ ìš”êµ¬ì‚¬í•­ì„ JSON í˜•íƒœë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ëŒ€í™” ë‚´ìš©: {full_conversation}

ë¨¼ì €, ì´ ëŒ€í™”ê°€ "ëŒ€í•™ ê³µì§€ì‚¬í•­ì„ ì°¾ëŠ” ì§ˆë¬¸"ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.
- ê³µì§€ì‚¬í•­ ê´€ë ¨: ì¥í•™ê¸ˆ, í•™ì‚¬ì¼ì •, ìˆ˜ê°•ì‹ ì²­, ì·¨ì—…, í–‰ì‚¬, í”„ë¡œê·¸ë¨, ì‹ ì²­, ëª¨ì§‘ ë“±
- ê³µì§€ì‚¬í•­ ë¬´ê´€: ì¸ì‚¬ë§, ì¡ë‹´, ì¼ìƒ ëŒ€í™”, ê³µì§€ì™€ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸

ë‹¤ìŒ í˜•íƒœì˜ JSONìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "is_notice_related": true ë˜ëŠ” false,
    "category": "ì¥í•™ê¸ˆ|í•™ì‚¬ì¼ì •|ìˆ˜ê°•ì‹ ì²­|ì·¨ì—…|í–‰ì‚¬|ê¸°íƒ€",
    "keywords": ["í•µì‹¬í‚¤ì›Œë“œ1", "í•µì‹¬í‚¤ì›Œë“œ2", ...],
    "target_audience": "í•™ë¶€ìƒ|ëŒ€í•™ì›ìƒ|ì „ì²´|íŠ¹ì •í•™ê³¼",
    "urgency": "ë†’ìŒ|ë³´í†µ|ë‚®ìŒ",
    "specific_requirements": "êµ¬ì²´ì ì¸ ìš”êµ¬ì‚¬í•­ ìš”ì•½"
}}

JSONë§Œ ì‘ë‹µí•˜ì„¸ìš”:"""

    llm = get_llm()
    response = llm.invoke(prompt)

    try:
        import json
        # JSON ì‘ë‹µì—ì„œ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
        content = response.content.strip()
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```jsonìœ¼ë¡œ ê°ì‹¸ì§„ ê²½ìš° ì²˜ë¦¬)
        if "```json" in content:
            start = content.find("```json") + 7
            end = content.find("```", start)
            content = content[start:end].strip()
        elif "```" in content:
            start = content.find("```") + 3
            end = content.find("```", start)
            content = content[start:end].strip()

        requirements = json.loads(content)

        # is_notice_related í•„ë“œ í™•ì¸ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ true)
        if "is_notice_related" not in requirements:
            requirements["is_notice_related"] = True

        # í•„ìˆ˜ í•„ë“œ ê²€ì¦ ë° ë³´ì™„
        if not requirements.get("keywords"):
            # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ì¶”ì¶œ
            important_words = []
            for msg in user_messages:
                words = [w for w in msg.split() if len(w) > 1 and w not in STOPWORDS]
                important_words.extend(words[:2])  # ê° ë©”ì‹œì§€ì—ì„œ 2ê°œì”©
            requirements["keywords"] = important_words[:5]  # ìµœëŒ€ 5ê°œ

        if not requirements.get("specific_requirements"):
            requirements["specific_requirements"] = full_conversation

        return requirements

    except Exception as parse_error:
        logging.warning("JSON parsing failed: %s", parse_error)
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë” ì •êµí•œ ê¸°ë³¸ê°’ ìƒì„±
        important_words = []
        for msg in user_messages:
            # ë¶ˆìš©ì–´ ì œê±°í•˜ê³  ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ì¶”ì¶œ
            words = [w for w in msg.split() if len(w) > 1 and w not in STOPWORDS]
            important_words.extend(words)

        # ì¤‘ë³µ ì œê±°í•˜ê³  ìƒìœ„ 5ê°œ ì„ íƒ
        unique_keywords = list(dict.fromkeys(important_words))[:5]

        return {
            "is_notice_related": True,  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
            "category": "ê¸°íƒ€",
            "keywords": unique_keywords,
            "target_audience": "ì „ì²´",
            "urgency": "ë³´í†µ",
            "specific_requirements": full_conversation[:200]  # ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ ì œí•œ
        }

# ===== ì¼ë°˜ ëŒ€í™” ì‘ë‹µ ìƒì„± =====
def generate_casual_response(conversation_history: List[Dict[str, str]]) -> str:
    """ê³µì§€ì‚¬í•­ê³¼ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ"""

    recent_conversation = ' '.join([f"{msg['role']}: {msg['content']}" for msg in conversation_history[-4:]])

    prompt = f"""ë‹¹ì‹ ì€ ì„œìš¸ì‹œë¦½ëŒ€í•™êµ ê³µì§€ì‚¬í•­ ì•ˆë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.

ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”:
{recent_conversation}

ì‚¬ìš©ìê°€ ê³µì§€ì‚¬í•­ê³¼ ê´€ë ¨ ì—†ëŠ” ë§ì„ í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ ì‘ë‹µí•˜ì„¸ìš”:

1. ì¸ì‚¬ë§ì´ë©´ ì¹œê·¼í•˜ê²Œ ì¸ì‚¬í•˜ê³ , ê³µì§€ì‚¬í•­ ê´€ë ¨ ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‹¤ê³  ì•ˆë‚´
2. ì¼ìƒ ëŒ€í™”ë‚˜ ì¡ë‹´ì´ë©´ ê°€ë³ê²Œ ì‘ëŒ€í•˜ê³ , ê³µì§€ì‚¬í•­ì´ í•„ìš”í•˜ë©´ ë„ì™€ì¤„ ìˆ˜ ìˆë‹¤ê³  ì•ˆë‚´
3. ì±—ë´‡ì´ í•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì´ë©´ ì •ì¤‘íˆ ë²”ìœ„ë¥¼ ì„¤ëª…í•˜ê³ , ê³µì§€ì‚¬í•­ ê´€ë ¨ ì§ˆë¬¸ì„ ìœ ë„
4. 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ë‹µí•  ê²ƒ
5. ë„ˆë¬´ ë”±ë”±í•˜ì§€ ì•Šê³  ì¹œê·¼í•œ í†¤ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ

ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ ì‘ë‹µì„ ì‘ì„±í•´ì£¼ì„¸ìš”:"""

    llm = get_llm()
    response = llm.invoke(prompt)
    return response.content.strip()

# ===== ì§ˆë¬¸ ìƒì„± =====
def generate_clarifying_question(turn: int, conversation_history: List[Dict[str, str]]) -> str:
    """ëŒ€í™” í„´ì— ê´€ê³„ì—†ì´ ìƒí™©ì— ë§ëŠ” ëª…í™•í™” ì§ˆë¬¸ ìƒì„±"""

    user_messages = [msg["content"] for msg in conversation_history if msg["role"] == "user"]

    # ì „ì²´ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•œ ì§ˆë¬¸ ìƒì„±
    recent_conversation = ' '.join([f"{msg['role']}: {msg['content']}" for msg in conversation_history[-6:]])

    prompt = f"""ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”:
{recent_conversation}

í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê³µì§€ì‚¬í•­ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ íŒŒì•…í•˜ê¸° ìœ„í•œ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ì¡°ê±´:
- 1ë¬¸ì¥ìœ¼ë¡œë§Œ ì‘ì„±í•  ê²ƒ
- ë„ˆë¬´ ê³µì‹ì ì´ì§€ ì•Šê³ , ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ í†¤ìœ¼ë¡œ ì§ˆë¬¸í•  ê²ƒ
- í•„ìš”í•˜ë‹¤ë©´ ì¹´í…Œê³ ë¦¬, ëŒ€ìƒ, ì‹œê¸°, ì¶”ê°€ ì •ë³´ ì¤‘ í•˜ë‚˜ë§Œ ê³¨ë¼ ì§ˆë¬¸í•  ê²ƒ
- ë¶ˆí•„ìš”í•˜ê²Œ ì—¬ëŸ¬ í•­ëª©ì„ í•œ ë²ˆì— ë¬»ì§€ ë§ ê²ƒ

ë‹¤ìŒê³¼ ê°™ì€ ë°©í–¥ìœ¼ë¡œ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
1. ì¹´í…Œê³ ë¦¬ í™•ì¸ (ì¥í•™ê¸ˆ, í•™ì‚¬ì¼ì •, ìˆ˜ê°•ì‹ ì²­, ì·¨ì—…ì •ë³´, í–‰ì‚¬ ë“±)
2. ëŒ€ìƒ í™•ì¸ (í•™ë¶€ìƒ, ëŒ€í•™ì›ìƒ, íŠ¹ì • í•™ê³¼)
3. ì‹œê¸° í™•ì¸ (ì–¸ì œ í•„ìš”í•œì§€)
4. ì¶”ê°€ ì •ë³´ ìš”ì²­

1,2,3,4ë²ˆì˜ ë‚´ìš© ì¤‘ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•˜ì—¬ ì ì ˆí•œ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ëª¨ë“  í•­ëª©ì´ í¬í•¨ë  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤.

ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í†¤ìœ¼ë¡œ 1ë¬¸ì¥ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”:"""

    llm = get_llm()
    response = llm.invoke(prompt)
    return response.content.strip()


def cosine_sim(a, b, eps: float = 1e-10) -> float:
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    dot = sum(x*y for x, y in zip(a, b))
    na = sqrt(sum(x*x for x in a)) + eps
    nb = sqrt(sum(y*y for y in b)) + eps
    return dot / (na * nb)



# ===== ìµœì¢… ê³µì§€ ì¶”ì²œ (ê³ ê¸‰ ê²€ìƒ‰ ë¡œì§ ì ìš©) =====
def rerank_documents(query: str, docs: List[Document]) -> List[Tuple[Document, float]]:
    """ì…ë ¥ ì¿¼ë¦¬ì™€ ë¬¸ì„œë“¤ì„ Cohere Rerankerë¡œ ì¬ì •ë ¬ (ë‚ ì§œ ì •ë³´ í¬í•¨)"""

    if not docs or not USE_RERANKER:
        # Reranker ë¹„í™œì„±í™” ì‹œ ê¸°ë³¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš© (ë°°ì¹˜ ì²˜ë¦¬ë¡œ ìµœì í™”)
        embeddings = get_embeddings()
        query_embedding = embeddings.embed_query(query)

        # ë°°ì¹˜ë¡œ ë¬¸ì„œ ì„ë² ë”© ê³„ì‚° (ë” ë¹ ë¦„)
        doc_texts = [doc.page_content for doc in docs]
        doc_embeddings = embeddings.embed_documents(doc_texts)

        results = []
        for doc, doc_embedding in zip(docs, doc_embeddings):
            score = cosine_sim(query_embedding, doc_embedding)
            results.append((doc, float(score)))

        return sorted(results, key=lambda x: x[1], reverse=True)

    cohere_client = get_cohere_client()
    if not cohere_client:
        logging.warning("Cohere client not available, using cosine similarity")
        # ì§ì ‘ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì¬ê·€ í˜¸ì¶œ ë°©ì§€)
        embeddings = get_embeddings()
        query_embedding = embeddings.embed_query(query)

        # ë°°ì¹˜ë¡œ ë¬¸ì„œ ì„ë² ë”© ê³„ì‚° (ë” ë¹ ë¦„)
        doc_texts = [doc.page_content for doc in docs]
        doc_embeddings = embeddings.embed_documents(doc_texts)

        results = []
        for doc, doc_embedding in zip(docs, doc_embeddings):
            score = cosine_sim(query_embedding, doc_embedding)
            results.append((doc, float(score)))

        return sorted(results, key=lambda x: x[1], reverse=True)

    try:
        # í˜„ì¬ ë‚ ì§œ ì •ë³´ë¥¼ ì¿¼ë¦¬ì— ì¶”ê°€
        now = datetime.now()
        enhanced_query = f"{query} (í˜„ì¬ ë‚ ì§œ: {now.strftime('%Yë…„ %mì›”')})"

        # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¤€ë¹„ (ë‚ ì§œ ì •ë³´ í¬í•¨)
        documents = []
        for doc in docs:
            metadata = doc.metadata or {}
            title = metadata.get("title", "")
            content = doc.page_content[:500]  # ë„ˆë¬´ ê¸´ ë¬¸ì„œ ì œí•œ
            posted_date = metadata.get("posted_date", "")

            # ë‚ ì§œ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ì— ëª…ì‹œ
            if posted_date:
                date_info = f"[ê²Œì‹œì¼: {posted_date}]"
            else:
                date_info = ""

            # ë‚ ì§œ + ì œëª© + ë‚´ìš© ê²°í•©
            if title:
                combined_text = f"{date_info} {title}\n{content}"
            else:
                combined_text = f"{date_info} {content}"

            documents.append(combined_text)

        # Cohere Rerank API í˜¸ì¶œ
        response = cohere_client.rerank(
            model="rerank-multilingual-v3.0",  # ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸
            query=enhanced_query,  # ë‚ ì§œ ì •ë³´ê°€ í¬í•¨ëœ ì¿¼ë¦¬
            documents=documents,
            top_n=min(FINAL_TOP_K, len(docs)),
            return_documents=True
        )

        # ê²°ê³¼ ì²˜ë¦¬
        reranked_results = []
        for result in response.results:
            original_doc = docs[result.index]
            relevance_score = result.relevance_score
            reranked_results.append((original_doc, float(relevance_score)))

        logging.info("[RERANK] Reranked %d documents, top score: %.3f",
                    len(reranked_results),
                    reranked_results[0][1] if reranked_results else 0.0)

        return reranked_results

    except Exception as e:
        logging.error("Cohere reranking failed: %s", e)
        # Fallback to cosine similarity (ì§ì ‘ ê³„ì‚°ìœ¼ë¡œ ì¬ê·€ ë°©ì§€)
        try:
            embeddings = get_embeddings()
            query_embedding = embeddings.embed_query(query)

            # ë°°ì¹˜ë¡œ ë¬¸ì„œ ì„ë² ë”© ê³„ì‚°
            doc_texts = [doc.page_content for doc in docs]
            doc_embeddings = embeddings.embed_documents(doc_texts)

            results = []
            for doc, doc_embedding in zip(docs, doc_embeddings):
                score = cosine_sim(query_embedding, doc_embedding)
                results.append((doc, float(score)))

            return sorted(results, key=lambda x: x[1], reverse=True)
        except Exception as fallback_error:
            logging.error("Fallback cosine similarity also failed: %s", fallback_error)
            return []

def find_best_notice(requirements: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """ìš”êµ¬ì‚¬í•­ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ì í•©í•œ ê³µì§€ 1ê°œ ì¶”ì²œ (Cohere Reranker ì‚¬ìš©)"""

    try:
        # 1) ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± - specific_req ì§ì ‘ ì‚¬ìš© (clean ê³¼ì • ì œê±°)
        specific_req = requirements.get('specific_requirements', '')

        # specific_reqë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ê°€ì¥ ì •í™•í•œ ì¿¼ë¦¬)
        if specific_req and specific_req.strip():
            search_query = specific_req.strip()
        else:
            # fallback: keywords ì‚¬ìš©
            keywords = requirements.get('keywords', [])
            if keywords:
                search_query = ' '.join(keywords)
            else:
                search_query = ""

        logging.info("[RECOMMEND] specific_req=%s", specific_req[:100])

        if not search_query:
            logging.warning("[RECOMMEND] Empty search query")
            return None

        logging.info("[RECOMMEND] search_query=%r", search_query)

        # 2) ë²¡í„°ìŠ¤í† ì–´ ê²€ì¦
        try:
            vectorstore = get_vectorstore()
            if not vectorstore:
                logging.error("[RECOMMEND] Vectorstore not available")
                return None
        except Exception as vs_error:
            logging.error("[RECOMMEND] Vectorstore initialization failed: %s", vs_error)
            return None

        # 3) ì´ˆê¸° ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰
        try:
            docs = vectorstore.similarity_search(
                search_query,
                k=INITIAL_SEARCH_K
            )
        except Exception as search_error:
            logging.error("[RECOMMEND] Similarity search failed: %s", search_error)
            return None

        if not docs:
            logging.info("[RECOMMEND] No documents found for query")
            return None

        # 4) Cohere Rerankerë¡œ ì¬ì •ë ¬ (ì•ˆì „í•œ í˜¸ì¶œ)
        try:
            reranked_results = rerank_documents(search_query, docs)
        except Exception as rerank_error:
            logging.error("[RECOMMEND] Reranking failed: %s", rerank_error)
            return None

        if not reranked_results:
            logging.info("[RECOMMEND] No results after reranking")
            return None

        # 5) ìµœê³  ì ìˆ˜ ë¬¸ì„œ ì„ íƒ
        best_doc, best_score = reranked_results[0]

        # ìƒìœ„ ê²°ê³¼ë“¤ ë¡œê¹… (ë””ë²„ê¹…ìš©)
        logging.info("[RECOMMEND] Top 3 results:")
        for i, (doc, score) in enumerate(reranked_results[:3]):
            title = doc.metadata.get("title", "ì œëª©ì—†ìŒ")[:50]
            logging.info("  %d. Score: %.3f | Title: %s", i+1, score, title)

        # 6) ì„ê³„ê°’ í™•ì¸
        if best_score < RERANK_THRESHOLD:
            logging.info("[RECOMMEND] Score below threshold: %.3f < %.3f", best_score, RERANK_THRESHOLD)
            return None

        # 7) ê²°ê³¼ êµ¬ì„± (ì•ˆì „í•œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ)
        try:
            metadata = best_doc.metadata or {}
            content = best_doc.page_content or ""

            return {
                "content": content,
                "score": float(best_score),
                "title": metadata.get("title", "ì œëª© ì—†ìŒ"),
                "link": metadata.get("link", ""),
                "posted_date": metadata.get("posted_date", ""),
                "department": metadata.get("department", ""),
                "category": metadata.get("category", ""),
                "doc_id": metadata.get("doc_id", "")
            }
        except Exception as result_error:
            logging.error("[RECOMMEND] Result construction failed: %s", result_error)
            return None

    except Exception as e:
        logging.error("[RECOMMEND] Unexpected error in find_best_notice: %s", e, exc_info=True)
        return None

def generate_final_recommendation(requirements: Dict[str, Any], notice: Dict[str, Any], conversation_history: List[Dict[str, str]]) -> str:
    """ìµœì¢… ì¶”ì²œ ë©”ì‹œì§€ ìƒì„±"""

    conversation_summary = " ".join([msg["content"] for msg in conversation_history if msg["role"] == "user"])

    # í˜„ì¬ ë‚ ì§œ ì •ë³´ ì¶”ê°€
    now = datetime.now()
    current_date_str = now.strftime("%Yë…„ %mì›” %dì¼")

    prompt = f"""**í˜„ì¬ ë‚ ì§œ: {current_date_str}**

ì‚¬ìš©ìê°€ ë‹¤ìŒê³¼ ê°™ì´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤: "{conversation_summary}"

ì´ì— ëŒ€í•œ ë‹µë³€ìœ¼ë¡œ ì í•©í•œ ê³µì§€ì‚¬í•­ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:

**ê³µì§€ì‚¬í•­ ì •ë³´:**
- ì œëª©: {notice.get('title')}
- ì£¼ê´€: {notice.get('department')}
- ê²Œì‹œì¼: {notice.get('posted_date')}
- ë‚´ìš©: {notice.get('content')[:1000]}

**ì§€ì¹¨:**
1. ê¸°ê°„ì— ëŒ€í•œ ë‹µë³€ì„ í•  ê²½ìš°, **í˜„ì¬ ë‚ ì§œë¥¼ ê³ ë ¤í•˜ì—¬** ëª¨ì§‘/ì‹ ì²­ ê¸°ê°„ì´ ì§€ë‚¬ëŠ”ì§€, ì§„í–‰ ì¤‘ì¸ì§€, ì˜ˆì •ì¸ì§€ ëª…í™•íˆ ì•Œë ¤ì£¼ì„¸ìš”
2. ì‚¬ìš©ìì˜ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
3. ê³µì§€ì‚¬í•­ì˜ ë‚´ìš©ì—ì„œ ì‚¬ìš©ìê°€ ê¶ê¸ˆí•´í•˜ëŠ” ë¶€ë¶„ì„ ì¤‘ì ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”
4. í˜•ì‹ì ì¸ "í–‰ì‚¬:", "ì¥ì†Œ:" ê°™ì€ êµ¬ì¡°í™”ëœ ë‹µë³€ ê¸ˆì§€
5. ìì—°ìŠ¤ëŸ½ê³  ëŒ€í™”ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
6. ì‚¬ìš©ìê°€ ì•Œê³  ì‹¶ì–´í•˜ëŠ” í•µì‹¬ ì •ë³´(ì–¸ì œ, ì–´ë””ì„œ, ëˆ„ê°€, ì–´ë–»ê²Œ)ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•˜ì„¸ìš”
7. ë§ˆì§€ë§‰ì— ë§í¬: {notice.get('link')} ì œê³µ

**ì˜ˆì‹œ í†¤:**
- ê¸°ê°„ì´ ì§€ë‚¬ë‹¤ë©´: "ì•„ì‰½ê²Œë„ ë„¤ì´ë²„ í´ë¼ìš°ë“œ ì•„ì¹´ë°ë¯¸ 1ê¸° ëª¨ì§‘ì€ 9ì›” 26ì¼ì— ë§ˆê°ë˜ì—ˆì–´ìš”. í•˜ì§€ë§Œ ë‹¤ìŒ ê¸°ìˆ˜ ëª¨ì§‘ ì •ë³´ëŠ”..."
- ì§„í–‰ ì¤‘ì´ë¼ë©´: "ë„¤ì´ë²„ í´ë¼ìš°ë“œ ì•„ì¹´ë°ë¯¸ 1ê¸° ëª¨ì§‘ì´ 9ì›” 26ì¼ê¹Œì§€ ì§„í–‰ë˜ê³  ìˆì–´ìš”. í˜„ì¬ ëª¨ì§‘ ì¤‘ì´ë‹ˆ ì„œë‘˜ëŸ¬ ì‹ ì²­í•˜ì„¸ìš”!"

ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:"""

    llm = get_llm()
    response = llm.invoke(prompt)
    return response.content.strip()

# ===== ì•± ì´ˆê¸°í™” ë° ëª¨ë¸ í”„ë¦¬ë¡œë”© =====
def preload_models():
    """ì•± ì‹œì‘ ì‹œ ëª¨ë“  ëª¨ë¸ì„ ë¯¸ë¦¬ ë¡œë“œí•˜ì—¬ ì²« ìš”ì²­ ì†ë„ ê°œì„ """
    try:
        logging.info("ğŸš€ Preloading models...")
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        get_embeddings()
        # ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
        get_vectorstore()
        # Cohere í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        get_cohere_client()
        # LLM ì´ˆê¸°í™”
        get_llm()
        logging.info("âœ… All models preloaded successfully!")
    except Exception as e:
        logging.error("âŒ Error preloading models: %s", e)

# ===== FastAPI ì•± =====
app = FastAPI(title="Notice Recommendation Chatbot", version="1.0.0")

@app.on_event("startup")
async def startup_event():
    """ì•± ì‹œì‘ ì‹œ ëª¨ë¸ í”„ë¦¬ë¡œë”©"""
    preload_models()

class ChatRequest(BaseModel):
    query: str
    session_id: str

class ChatResponse(BaseModel):
    response: str
    turn: int
    completed: bool
    recommended_notice: Optional[Dict[str, Any]] = None

@app.get("/health")
def health_check():
    """í—¬ìŠ¤ì²´í¬"""
    try:
        _ = get_vectorstore()
        return {"status": "healthy", "timestamp": datetime.now().isoformat()}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}

@app.post("/chat", response_model=ChatResponse)
def chat(request: ChatRequest):
    """ëŒ€í™”í˜• ê³µì§€ ì¶”ì²œ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        session_id = request.session_id
        state = conversation_state[session_id]

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        state["messages"].append({
            "role": "user",
            "content": request.query,
            "timestamp": datetime.now().isoformat()
        })
        state["turns"] += 1

        # ë§¤ í„´ë§ˆë‹¤ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
        requirements = extract_requirements(state["messages"])
        state["user_requirements"] = requirements

        # ê³µì§€ì‚¬í•­ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ í™•ì¸
        is_notice_related = requirements.get("is_notice_related", True)

        if not is_notice_related:
            # ê³µì§€ì‚¬í•­ê³¼ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸: ì¼ë°˜ ëŒ€í™” ì‘ë‹µ
            casual_response = generate_casual_response(state["messages"])

            # ì±—ë´‡ ì‘ë‹µ ì¶”ê°€
            state["messages"].append({
                "role": "assistant",
                "content": casual_response,
                "timestamp": datetime.now().isoformat()
            })

            return ChatResponse(
                response=casual_response,
                turn=state["turns"],
                completed=False,
                recommended_notice=None
            )

        # ê³µì§€ì‚¬í•­ ê´€ë ¨ ì§ˆë¬¸: ìµœì  ê³µì§€ ì°¾ê¸°
        best_notice = find_best_notice(requirements)

        if best_notice:
            # ê³µì§€ë¥¼ ì°¾ì•˜ì„ ë•Œ: ìµœì¢… ì¶”ì²œ ë©”ì‹œì§€ ìƒì„±
            final_response = generate_final_recommendation(requirements, best_notice, state["messages"])

            # ì±—ë´‡ ì‘ë‹µ ì¶”ê°€
            state["messages"].append({
                "role": "assistant",
                "content": final_response,
                "timestamp": datetime.now().isoformat()
            })

            return ChatResponse(
                response=final_response,
                turn=state["turns"],
                completed=False,  # ëŒ€í™” ê³„ì† ê°€ëŠ¥
                recommended_notice=best_notice
            )
        else:
            # ê³µì§€ë¥¼ ì°¾ì§€ ëª»í–ˆì„ ë•Œ: ëª…í™•í™” ì§ˆë¬¸ ìƒì„±
            clarifying_question = generate_clarifying_question(state["turns"], state["messages"])

            # ì±—ë´‡ ì‘ë‹µ ì¶”ê°€
            state["messages"].append({
                "role": "assistant",
                "content": clarifying_question,
                "timestamp": datetime.now().isoformat()
            })

            return ChatResponse(
                response=clarifying_question,
                turn=state["turns"],
                completed=False,
                recommended_notice=None
            )

    except Exception as e:
        return ChatResponse(
            response=f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            turn=0,
            completed=False,
            recommended_notice=None
        )

@app.get("/session/{session_id}")
def get_session_info(session_id: str):
    """ì„¸ì…˜ ì •ë³´ ì¡°íšŒ"""
    state = conversation_state.get(session_id)
    if not state:
        return {"error": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    return {
        "session_id": session_id,
        "turns": state["turns"],
        "completed": state["completed"],
        "messages": state["messages"],
        "user_requirements": state.get("user_requirements", {})
    }

@app.delete("/session/{session_id}")
def reset_session(session_id: str):
    """ì„¸ì…˜ ì´ˆê¸°í™”"""
    if session_id in conversation_state:
        del conversation_state[session_id]
    return {"message": f"ì„¸ì…˜ {session_id}ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤."}

@app.get("/sessions")
def get_active_sessions():
    """í™œì„± ì„¸ì…˜ ëª©ë¡"""
    sessions = []
    for session_id, state in conversation_state.items():
        sessions.append({
            "session_id": session_id,
            "turns": state["turns"],
            "completed": state["completed"],
            "last_activity": state["messages"][-1]["timestamp"] if state["messages"] else None
        })
    return {"sessions": sessions}
