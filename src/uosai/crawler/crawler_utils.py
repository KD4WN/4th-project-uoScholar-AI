# src/uosai/crawler/crawler_utils.py
"""
ê³µí†µ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ
- DB ì—°ê²° ë° CRUD
- OpenAI API (ìš”ì•½, ì„ë² ë”©)
- Playwright ìŠ¤í¬ë¦°ìƒ·
- HTML íŒŒì‹± ìœ í‹¸
"""

import os
import re
import sys
import traceback
from contextlib import contextmanager
from typing import Optional, List
from datetime import datetime
from io import BytesIO
import base64

import requests
from bs4 import BeautifulSoup
import mysql.connector
from mysql.connector import Error as MySQLError

from openai import OpenAI
from PIL import Image

from dotenv import load_dotenv
load_dotenv()

# Playwright
try:
    from playwright.sync_api import sync_playwright
    _PLAYWRIGHT_AVAILABLE = True
except Exception:
    _PLAYWRIGHT_AVAILABLE = False

# =========================
# í™˜ê²½ì„¤ì •
# =========================
BASE_DIR = os.path.abspath(os.getcwd())
OUT_DIR = os.path.join(BASE_DIR, "screenshot")
os.makedirs(OUT_DIR, exist_ok=True)

DB_CONFIG = {
    "host": os.getenv("DB_HOST"),
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASSWORD"),
    "database": os.getenv("DB_NAME"),
    "port": int(os.getenv("DB_PORT", "3306")),
    "charset": os.getenv("DB_CHARSET", "utf8mb4"),
    "autocommit": os.getenv("DB_AUTOCOMMIT", "False") == "True",
    "use_pure": os.getenv("DB_USE_PURE", "True") == "True",
    "connection_timeout": int(os.getenv("DB_CONN_TIMEOUT", 20)),
    "raise_on_warnings": os.getenv("DB_WARNINGS", "True") == "True",
}

# OpenAI
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY").strip()
client = OpenAI(api_key=OPENAI_API_KEY)
SUMMARIZE_MODEL = "gpt-4o"

PLAYWRIGHT_TIMEOUT_MS = 90000
CONNECT_TIMEOUT = 10
READ_TIMEOUT = 20

# =========================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =========================

def log(msg: str) -> None:
    """ë¡œê·¸ ì¶œë ¥"""
    print(f"[crawler {datetime.now():%Y-%m-%d %H:%M:%S}] {msg}")


@contextmanager
def mysql_conn():
    """MySQL ì—°ê²° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    conn = mysql.connector.connect(**DB_CONFIG)
    try:
        yield conn
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()


def parse_date_yyyy_mm_dd(text: str) -> Optional[str]:
    """YYYY-MM-DD í˜•ì‹ ë‚ ì§œ ì¶”ì¶œ"""
    m = re.search(r"(\d{4}-\d{2}-\d{2})", text or "")
    return m.group(1) if m else None


def extract_main_text_from_html(html: str, max_chars: int = 12000) -> str:
    """HTMLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ"""
    soup = BeautifulSoup(html, "html.parser")

    # ë³¸ë¬¸ í›„ë³´ ì…€ë ‰í„°
    candidates = [
        "div.vw-cnt", "div.vw-con", "div.vw-bd", "div.board-view",
        "article", "div#content", "div#contents", "main"
    ]
    main = None
    for sel in candidates:
        node = soup.select_one(sel)
        if node and node.get_text(strip=True):
            main = node
            break
    if main is None:
        main = soup.body or soup

    # ë¶ˆí•„ìš” ì˜ì—­ ì œê±°
    kill_selectors = [
        ".related", ".relate", ".attach", ".file", ".files",
        ".prev", ".next", "footer", "#footer", ".sns", ".share",
        ".copyright", ".copy", ".address", ".addr"
    ]
    for ks in kill_selectors:
        for n in main.select(ks):
            n.decompose()

    text = main.get_text("\n", strip=True)

    # í‘¸í„°/ì£¼ì†Œ/ì¹´í”¼ë¼ì´íŠ¸ ë¬¸êµ¬ ì œê±°
    drop_patterns = [
        r"ì„œìš¸ì‹œë¦½ëŒ€í•™êµ\s*.+?\d{2,3}-\d{3,4}-\d{4}",
        r"Copyright.+?All rights reserved\.?",
        r"ì´ì „ê¸€.*", r"ë‹¤ìŒê¸€.*", r"ê´€ë ¨\s?ê²Œì‹œë¬¼.*",
    ]
    for pat in drop_patterns:
        text = re.sub(pat, "", text, flags=re.I | re.S)

    # ê³µë°± ì •ë¦¬
    text = re.sub(r"\n{3,}", "\n\n", text).strip()

    # ê³¼ë„í•œ ê¸¸ì´ ì œí•œ
    if len(text) > max_chars:
        text = text[:max_chars] + "\n\n[... ë³¸ë¬¸ ì¼ë¶€ ìƒëµ ...]"

    return text


# =========================
# Playwright ìŠ¤í¬ë¦°ìƒ·
# =========================

def html_to_images_playwright(
    url: str,
    viewport_width: int = 1200,
    slice_height: int = 1920,
    timeout_ms: int = PLAYWRIGHT_TIMEOUT_MS,
    debug_full_image_path: Optional[str] = None,
    full_image_format: str = "png",
) -> List[Image.Image]:
    """í˜ì´ì§€ ì „ì²´ë¥¼ ìŠ¤í¬ë¦°ìƒ· ì°ê³  slice_height ê°„ê²©ìœ¼ë¡œ ë¶„í• """
    if not _PLAYWRIGHT_AVAILABLE:
        print("âŒ Playwright ë¯¸ì„¤ì¹˜/ì„í¬íŠ¸ ì‹¤íŒ¨")
        return []

    imgs: List[Image.Image] = []
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,
                args=[
                    "--disable-web-security",
                    "--hide-scrollbars",
                    "--disable-blink-features=AutomationControlled",
                ]
            )
            page = browser.new_page(
                viewport={"width": viewport_width, "height": slice_height},
                device_scale_factor=2.0,
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                extra_http_headers={
                    "Accept-Language": "ko-KR,ko;q=0.9",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                }
            )
            page.goto(url, wait_until="networkidle", timeout=timeout_ms)

            try:
                page.wait_for_selector("div.vw-tibx", timeout=timeout_ms)
            except Exception:
                pass

            for _ in range(6):
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                page.wait_for_timeout(700)

            page.wait_for_load_state("domcontentloaded")
            page.wait_for_timeout(500)

            # ì „ì²´ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ·
            if full_image_format.lower() == "png":
                buf = page.screenshot(full_page=True, type="png")
            else:
                buf = page.screenshot(full_page=True, type="jpeg", quality=85)
            browser.close()

        # ì „ì²´ í˜ì´ì§€ ì €ì¥ (ë””ë²„ê·¸)
        if debug_full_image_path:
            try:
                with open(debug_full_image_path, "wb") as f:
                    f.write(buf)
                print(f"ğŸ’¾ Full screenshot saved: {debug_full_image_path}")
            except Exception as e:
                print(f"âš ï¸ Full screenshot save failed: {e}")

        # ìŠ¬ë¼ì´ìŠ¤ ë¶„í• 
        full_img = Image.open(BytesIO(buf)).convert("RGB")
        W, H = full_img.size
        y = 0
        while y < H:
            crop = full_img.crop((0, y, W, min(y + slice_height, H)))
            imgs.append(crop)
            y += slice_height

    except Exception as e:
        print(f"âŒ HTMLâ†’ì´ë¯¸ì§€ ìº¡ì²˜ ì‹¤íŒ¨: {e}")

    return imgs


# =========================
# OpenAI ìš”ì•½
# =========================

def pil_to_data_url(pil_image: Image.Image, fmt="JPEG", quality=80) -> str:
    """PIL ì´ë¯¸ì§€ë¥¼ Data URLë¡œ ë³€í™˜"""
    bio = BytesIO()
    pil_image.save(bio, format=fmt, quality=quality, optimize=True)
    b64 = base64.b64encode(bio.getvalue()).decode("utf-8")
    return f"data:image/{fmt.lower()};base64,{b64}"


def summarize_with_text_and_images(html_text: str, images: List[Image.Image]) -> str:
    """HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ë¡œ ìš”ì•½ ìƒì„±"""
    merge_prompt = f"""
ì•„ë˜ëŠ” ëŒ€í•™ ê³µì§€ì‚¬í•­ì˜ 'HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸'ì…ë‹ˆë‹¤. ì´ í…ìŠ¤íŠ¸ë¥¼ **ìš°ì„  ê·¼ê±°**ë¡œ ì‚¼ê³ ,
ì¶”ê°€ë¡œ ì œê³µë˜ëŠ” 'í˜ì´ì§€ ì „ì²´ ìº¡ì²˜ ì´ë¯¸ì§€ë“¤'ì—ì„œë§Œ ë³´ì´ëŠ” í‘œ/í¬ìŠ¤í„°/ìŠ¤ìº”ëœ ë¬¸ì¥ ë“± ëˆ„ë½ ì •ë³´ë¥¼ **ë³´ì™„**í•˜ì—¬
ë‚´ìš©ì„ ë§ë¶™ì—¬ì£¼ì„¸ìš”.

- ë³¸ë¬¸ê³¼ ë¬´ê´€í•œ ì‚¬ì´ë“œ/í‘¸í„°/ì£¼ì†Œ/ì¹´í”¼ë¼ì´íŠ¸/ê´€ë ¨ ê²Œì‹œë¬¼ ë“±ì€ ì œì™¸í•˜ì„¸ìš”.
- ìˆ˜ì¹˜ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ë³´ì¡´
- ë‚ ì§œ ë° ì‹œê°„ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë³´ì¡´
- ê¸°ê´€/ë¶€ì„œ, ì¥ì†Œ, ì „í™”, ë©”ì¼ì€ ì›ë¬¸ í‘œê¸° ê·¸ëŒ€ë¡œ ì‚¬ìš©(ì¶”ì¸¡ ê¸ˆì§€)
- "ì œê³µëœ HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ì¶”ê°€ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ê³µì§€ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:" ì™€ ê°™ì€, ê³µì§€ ì‚¬í•­ì˜ ë‚´ìš© ì´ì™¸ì˜ ë‹¤ë¥¸ ë©˜íŠ¸ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ë©´ ì•ˆë¨. ì •í™•íˆ ê³µì§€ì‚¬í•­ ë‚´ìš©'ë§Œ' í¬í•¨í•´ì•¼í•¨.

[HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì‹œì‘]
{html_text}
[HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸ ë]
""".strip()

    contents = [{"type": "input_text", "text": merge_prompt}]
    for img in images:
        contents.append({
            "type": "input_image",
            "image_url": pil_to_data_url(img, fmt="JPEG", quality=75)
        })
    try:
        resp = client.responses.create(
            model=SUMMARIZE_MODEL,
            input=[{"role": "user", "content": contents}],
            temperature=0.2,
        )
        return (resp.output_text or "").strip()
    except Exception as e:
        print(f"âŒ í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ìš”ì•½ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        traceback.print_exc(limit=2, file=sys.stdout)
        return ""


# =========================
# DB CRUD
# =========================

UPSERT_SQL = """
INSERT INTO notice
    (category, post_number, title, link, summary, embedding_vector, posted_date, department, view_count)
VALUES
    (%s, %s, %s, %s, %s, %s, %s, %s, %s) AS new
ON DUPLICATE KEY UPDATE
    title = new.title,
    link = new.link,
    summary = new.summary,
    embedding_vector = new.embedding_vector,
    posted_date = new.posted_date,
    department = new.department,
    view_count = new.view_count
"""

EXISTS_SQL = "SELECT posted_date FROM notice WHERE category=%s AND post_number=%s LIMIT 1"


def _normalize_post_number(post_number) -> int:
    """post_numberë¥¼ ì •ìˆ˜ë¡œ ì •ê·œí™” (slugëŠ” CRC32 í•´ì‹œ)"""
    if isinstance(post_number, int):
        return post_number

    import zlib
    hash_val = zlib.crc32(post_number.encode('utf-8')) & 0x7fffffff
    return hash_val


def get_existing_posted_date(category: str, post_number) -> Optional[str]:
    """ê¸°ì¡´ ê²Œì‹œë¬¼ì˜ posted_date ì¡°íšŒ"""
    post_num = _normalize_post_number(post_number)

    with mysql_conn() as conn:
        cur = conn.cursor()
        cur.execute(EXISTS_SQL, (category, post_num))
        row = cur.fetchone()
        cur.close()
        return row[0] if row else None


def upsert_notice(row: dict):
    """ê³µì§€ì‚¬í•­ DB ì—…ì„œíŠ¸"""
    with mysql_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            UPSERT_SQL,
            (
                row["category"],
                row["post_number"],
                row["title"],
                row["link"],
                row.get("summary") or None,
                row.get("embedding_vector") or None,
                row["posted_date"],
                row.get("department") or None,
                row.get("view_count") or 0,
            ),
        )
        cur.close()


def update_view_count(category: str, post_number, view_count: int):
    """ì¡°íšŒìˆ˜ë§Œ ì—…ë°ì´íŠ¸"""
    post_num = _normalize_post_number(post_number)
    with mysql_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            "UPDATE notice SET view_count = %s WHERE category = %s AND post_number = %s",
            (view_count, category, post_num)
        )
        cur.close()


def _ymd(x: Optional[object]) -> Optional[str]:
    """ë‚ ì§œ ê°ì²´ë¥¼ YYYY-MM-DD ë¬¸ìì—´ë¡œ ë³€í™˜"""
    if x is None:
        return None
    if isinstance(x, (datetime, )):
        return x.strftime("%Y-%m-%d")
    from datetime import date
    if isinstance(x, date):
        return x.strftime("%Y-%m-%d")
    s = str(x).strip()
    return s[:10]
