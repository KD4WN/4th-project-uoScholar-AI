# src/uosai/crawler/notice_crawler.py

import os
import time
import re
from contextlib import contextmanager
from typing import Optional, Dict, List

import base64
from io import BytesIO
from collections import OrderedDict
from urllib.parse import urlencode

import requests
from bs4 import BeautifulSoup
import mysql.connector
from mysql.connector import Error as MySQLError

from openai import OpenAI
from PIL import Image  # ì´ë¯¸ì§€ ì²˜ë¦¬
from datetime import date, datetime
import sys, traceback

from dotenv import load_dotenv
load_dotenv()

# Playwright
try:
    from playwright.sync_api import sync_playwright
    _PLAYWRIGHT_AVAILABLE = True
except Exception:
    _PLAYWRIGHT_AVAILABLE = False

# =========================
# 0) í™˜ê²½ì„¤ì •
# =========================
BASE_DIR = os.path.abspath(os.getcwd())
OUT_DIR = os.path.join(BASE_DIR, "screenshot")
os.makedirs(OUT_DIR, exist_ok=True)

DB_CONFIG = {
    "host": os.getenv("DB_HOST"),
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASSWORD"),
    "database": os.getenv("DB_NAME"),
    "port": int(os.getenv("DB_PORT", "3306")),
    "charset": os.getenv("DB_CHARSET", "utf8mb4"),
    "autocommit": os.getenv("DB_AUTOCOMMIT", "False") == "True",
    "use_pure": os.getenv("DB_USE_PURE", "True") == "True",
    "connection_timeout": int(os.getenv("DB_CONN_TIMEOUT", 20)),
    "raise_on_warnings": os.getenv("DB_WARNINGS", "True") == "True",
}

# OpenAI
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY").strip()
client = OpenAI(api_key=OPENAI_API_KEY)
SUMMARIZE_MODEL = "gpt-4o"

#################################################################################
# ì¹´í…Œê³ ë¦¬ â†” list_id ë§¤í•‘
CATEGORIES: Dict[str, str] = {
    "COLLEGE_ENGINEERING": "20013DA1",
    "COLLEGE_HUMANITIES": "human01",
    "COLLEGE_SOCIAL_SCIENCES": "econo01",
    "COLLEGE_URBAN_SCIENCE": "urbansciences01",
    "COLLEGE_ARTS_SPORTS": "artandsport01",
    "COLLEGE_BUSINESS": "20008N2",
    "COLLEGE_NATURAL_SCIENCES": "scien01",
    "COLLEGE_LIBERAL_CONVERGENCE": "clacds01",
    "GENERAL": "FA1",     
    "ACADEMIC": "FA2",    
}
#################################################################################

CRAWL_VIEW_URL = "https://www.uos.ac.kr/korNotice/view.do?identified=anonymous&"
CRAWL_LIST_URL = "https://www.uos.ac.kr/korNotice/list.do?identified=anonymous&"

SAVE_VIEW_URL = "https://www.uos.ac.kr/korNotice/view.do"

#################################################################################

CHEME_LIST_URL = "https://cheme.uos.ac.kr/bbs/board.php?bo_table=notice" #í™”í•™ê³µí•™ê³¼
LIFE_SCI_LIST_URL = "https://lifesci.uos.ac.kr/community/notice"         #ìƒëª…ê³¼í•™ê³¼

# ëª‡ ê°œ í¬ë¡¤ë§í•  ê±´ì§€ 
REQUEST_SLEEP = 1.0
PLAYWRIGHT_TIMEOUT_MS = 90000
RECENT_WINDOW = 50

# =========================
# 1) ìœ í‹¸
# =========================

# ë¡œê·¸ ì¶œë ¥
def log(msg: str) -> None:
    print(f"[indexer {datetime.now():%Y-%m-%d %H:%M:%S}] {msg}")

@contextmanager
def mysql_conn():
    conn = mysql.connector.connect(**DB_CONFIG)
    try:
        yield conn
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()


def parse_date_yyyy_mm_dd(text: str) -> Optional[str]:
    m = re.search(r"(\d{4}-\d{2}-\d{2})", text or "")
    return m.group(1) if m else None


def extract_main_text_from_html(html: str, max_chars: int = 12000) -> str:
    """
    ê³µì§€ì˜ 'ë³¸ë¬¸' ì»¨í…Œì´ë„ˆì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ.
    ì‚¬ì´ë“œ/í‘¸í„°/ê´€ë ¨ê¸€/ì£¼ì†Œ/ì¹´í”¼ë¼ì´íŠ¸ ë“±ì€ ì œê±°í•˜ê³ ,
    ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ë©´ max_charsë¡œ ì˜ë¼ ëª¨ë¸ ì…ë ¥ì„ ì•ˆì •í™”.
    """
    soup = BeautifulSoup(html, "html.parser")

    # ë³¸ë¬¸ í›„ë³´ ì…€ë ‰í„° (ì‚¬ì´íŠ¸ ë§ê²Œ í•„ìš”ì‹œ ì¶”ê°€)
    candidates = [
        "div.vw-cnt", "div.vw-con", "div.vw-bd", "div.board-view",
        "article", "div#content", "div#contents", "main"
    ]
    main = None
    for sel in candidates:
        node = soup.select_one(sel)
        if node and node.get_text(strip=True):
            main = node
            break
    if main is None:
        main = soup.body or soup

    # ë¶ˆí•„ìš” ì˜ì—­ ì œê±°
    kill_selectors = [
        ".related", ".relate", ".attach", ".file", ".files",
        ".prev", ".next", "footer", "#footer", ".sns", ".share",
        ".copyright", ".copy", ".address", ".addr"
    ]
    for ks in kill_selectors:
        for n in main.select(ks):
            n.decompose()

    text = main.get_text("\n", strip=True)

    # í”í•œ í‘¸í„°/ì£¼ì†Œ/ì¹´í”¼ë¼ì´íŠ¸ ë¬¸êµ¬ ì œê±°
    drop_patterns = [
        r"ì„œìš¸ì‹œë¦½ëŒ€í•™êµ\s*.+?\d{2,3}-\d{3,4}-\d{4}",
        r"Copyright.+?All rights reserved\.?",
        r"ì´ì „ê¸€.*", r"ë‹¤ìŒê¸€.*", r"ê´€ë ¨\s?ê²Œì‹œë¬¼.*",
    ]
    for pat in drop_patterns:
        text = re.sub(pat, "", text, flags=re.I | re.S)

    # ê³µë°± ì •ë¦¬
    text = re.sub(r"\n{3,}", "\n\n", text).strip()

    # ê³¼ë„í•œ ê¸¸ì´ ì œí•œ
    if len(text) > max_chars:
        text = text[:max_chars] + "\n\n[... ë³¸ë¬¸ ì¼ë¶€ ìƒëµ ...]"

    return text


# =========================
# 2) Playwrightë¡œ HTML â†’ ì´ë¯¸ì§€ ìº¡ì²˜
# =========================
def html_to_images_playwright(
    url: str,
    viewport_width: int = 1200,
    slice_height: int = 1920,
    timeout_ms: int = PLAYWRIGHT_TIMEOUT_MS,
    debug_full_image_path: Optional[str] = None,  # ì „ì²´ í˜ì´ì§€ 1ì¥ ì €ì¥ ê²½ë¡œ
    full_image_format: str = "png",               # "png"|"jpeg"
) -> List[Image.Image]:
    """
    í˜ì´ì§€ ì „ì²´ë¥¼ full_page ìŠ¤í¬ë¦°ìƒ·ìœ¼ë¡œ ì°ì€ ë’¤,
    slice_height ê°„ê²©ìœ¼ë¡œ ëê¹Œì§€ ì „ë¶€ ì˜ë¼ì„œ ë°˜í™˜.
    (max_slices ì œí•œ ì—†ìŒ)
    debug_full_image_pathê°€ ì£¼ì–´ì§€ë©´ ì „ì²´ ìŠ¤í¬ë¦°ìƒ· ì›ë³¸ì„ íŒŒì¼ë¡œ ì €ì¥.
    """
    if not _PLAYWRIGHT_AVAILABLE:
        print("âŒ Playwright ë¯¸ì„¤ì¹˜/ì„í¬íŠ¸ ì‹¤íŒ¨")
        return []

    imgs: List[Image.Image] = []
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True, args=[
                "--disable-web-security",
                "--hide-scrollbars",
            ])
            page = browser.new_page(
                viewport={"width": viewport_width, "height": slice_height},
                device_scale_factor=2.0,
            )
            page.goto(url, wait_until="domcontentloaded", timeout=timeout_ms)

            try:
                page.wait_for_selector("div.vw-tibx", timeout=timeout_ms)
            except Exception:
                pass

            for _ in range(6):
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                page.wait_for_timeout(700)

            page.wait_for_load_state("domcontentloaded")
            page.wait_for_timeout(500)

            # ì „ì²´ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ·
            if full_image_format.lower() == "png":
                buf = page.screenshot(full_page=True, type="png")
            else:
                buf = page.screenshot(full_page=True, type="jpeg", quality=85)
            browser.close()

        # ì „ì²´ í˜ì´ì§€ í•œ ì¥ ì €ì¥(í…ŒìŠ¤íŠ¸/ë””ë²„ê·¸)
        if debug_full_image_path:
            try:
                with open(debug_full_image_path, "wb") as f:
                    f.write(buf)
                print(f"ğŸ’¾ Full screenshot saved: {debug_full_image_path}")
            except Exception as e:
                print(f"âš ï¸ Full screenshot save failed: {e}")

        # ìŠ¬ë¼ì´ìŠ¤ ë¶„í• 
        full_img = Image.open(BytesIO(buf)).convert("RGB")
        W, H = full_img.size
        y = 0
        while y < H:
            crop = full_img.crop((0, y, W, min(y + slice_height, H)))
            imgs.append(crop)
            y += slice_height

    except Exception as e:
        print(f"âŒ HTMLâ†’ì´ë¯¸ì§€ ìº¡ì²˜ ì‹¤íŒ¨: {e}")

    return imgs


# =========================
# 3) OpenAI: ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ìš”ì•½ + ì„ë² ë”©
# =========================
def pil_to_data_url(pil_image: Image.Image, fmt="JPEG", quality=80) -> str:
    bio = BytesIO()
    pil_image.save(bio, format=fmt, quality=quality, optimize=True)
    b64 = base64.b64encode(bio.getvalue()).decode("utf-8")

    return f"data:image/{fmt.lower()};base64,{b64}"

def summarize_with_text_and_images(html_text: str, images: List[Image.Image]) -> str:
    """
    HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ìš°ì„  ê·¼ê±°ë¡œ ì‚¼ê³ ,
    ì´ë¯¸ì§€(í¬ìŠ¤í„°/í‘œ ë“±)ì—ë§Œ ìˆëŠ” ëˆ„ë½ ì •ë³´ë¥¼ ë³´ê°•í•˜ë„ë¡ ì§€ì‹œ.
    """
    merge_prompt = f"""
ì•„ë˜ëŠ” ëŒ€í•™ ê³µì§€ì‚¬í•­ì˜ 'HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸'ì…ë‹ˆë‹¤. ì´ í…ìŠ¤íŠ¸ë¥¼ **ìš°ì„  ê·¼ê±°**ë¡œ ì‚¼ê³ ,
ì¶”ê°€ë¡œ ì œê³µë˜ëŠ” 'í˜ì´ì§€ ì „ì²´ ìº¡ì²˜ ì´ë¯¸ì§€ë“¤'ì—ì„œë§Œ ë³´ì´ëŠ” í‘œ/í¬ìŠ¤í„°/ìŠ¤ìº”ëœ ë¬¸ì¥ ë“± ëˆ„ë½ ì •ë³´ë¥¼ **ë³´ì™„**í•˜ì—¬
ë‚´ìš©ì„ ë§ë¶™ì—¬ì£¼ì„¸ìš”.

- ë³¸ë¬¸ê³¼ ë¬´ê´€í•œ ì‚¬ì´ë“œ/í‘¸í„°/ì£¼ì†Œ/ì¹´í”¼ë¼ì´íŠ¸/ê´€ë ¨ ê²Œì‹œë¬¼ ë“±ì€ ì œì™¸í•˜ì„¸ìš”.
- ìˆ˜ì¹˜ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ë³´ì¡´
- ë‚ ì§œ ë° ì‹œê°„ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë³´ì¡´
- ê¸°ê´€/ë¶€ì„œ, ì¥ì†Œ, ì „í™”, ë©”ì¼ì€ ì›ë¬¸ í‘œê¸° ê·¸ëŒ€ë¡œ ì‚¬ìš©(ì¶”ì¸¡ ê¸ˆì§€) 
- "ì œê³µëœ HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ì¶”ê°€ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ê³µì§€ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:" ì™€ ê°™ì€, ê³µì§€ ì‚¬í•­ì˜ ë‚´ìš© ì´ì™¸ì˜ ë‹¤ë¥¸ ë©˜íŠ¸ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ë©´ ì•ˆë¨. ì •í™•íˆ ê³µì§€ì‚¬í•­ ë‚´ìš©'ë§Œ' í¬í•¨í•´ì•¼í•¨.

[HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì‹œì‘]
{html_text}
[HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸ ë]
""".strip()

    contents = [{"type": "input_text", "text": merge_prompt}]
    for img in images:
        contents.append({
            "type": "input_image",
            "image_url": pil_to_data_url(img, fmt="JPEG", quality=75)  # JPEGë¡œ ì••ì¶•
        })
    try:
        resp = client.responses.create(
            model=SUMMARIZE_MODEL,   # "gpt-4o" ê¶Œì¥
            input=[{"role": "user", "content": contents}],
            temperature=0.2,
        )
        return (resp.output_text or "").strip()
    except Exception as e:
        print(f"âŒ í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ìš”ì•½ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        traceback.print_exc(limit=2, file=sys.stdout)
        return ""

# =========================
# 4) HTML íŒŒì‹± (ìƒì„¸)
# =========================
CONNECT_TIMEOUT = 10    # ì„œë²„ TCP ì—°ê²°ê¹Œì§€ ê¸°ë‹¤ë¦´ ìµœëŒ€ ì‹œê°„
READ_TIMEOUT    = 20   # ì‹¤ì œ ì‘ë‹µ(HTML)ì„ ë°›ëŠ” ì‹œê°„

def fetch_notice_html(list_id: str, seq: int) -> Optional[str]:
    try:
        params = {
            "list_id": list_id,
            "seq": str(seq),
            "sort": "1",
            "pageIndex": "1",
            "searchCnd": "",
            "searchWrd": "",
            "cate_id": "",
            "viewAuth": "Y",
            "writeAuth": "Y",
            "board_list_num": "10",
            "lpageCount": "12",
            "menuid": "",
        }
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(CRAWL_VIEW_URL, params=params, headers=headers, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT))
        if r.status_code != 200:
            print(f"âŒ HTTP {r.status_code} for seq={seq}")
            return None
        return r.text
    except Exception as e:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨ seq={seq}: {e}")
        return None


def parse_notice_fields(html: str, seq: int) -> Optional[dict]:
    soup = BeautifulSoup(html, "html.parser")
    title_el = soup.select_one("div.vw-tibx h4") if soup else None
    title = title_el.get_text(strip=True) if title_el else ""
    if not title:
        return None  # ê²Œì‹œë¬¼ ì—†ìŒ

    spans = soup.select("div.vw-tibx div.zl-bx div.da span")
    department = spans[1].get_text(strip=True) if len(spans) >= 3 else ""
    date_text = spans[2].get_text(strip=True) if len(spans) >= 3 else ""
    dt = parse_date_yyyy_mm_dd(date_text) or datetime.now().strftime("%Y-%m-%d")

    post_number_el = soup.select_one("input[name=seq]")
    post_number = int(post_number_el["value"]) if post_number_el and post_number_el.get("value") else int(seq)

    content_text = soup.get_text("\n", strip=True)
    return {
        "title": title,
        "department": department,
        "posted_date": dt,
        "post_number": post_number,
        "content_text": content_text,
    }


# =========================
# 5) DB ì—…ì„œíŠ¸
# =========================
UPSERT_SQL = """
INSERT INTO notice
    (category, post_number, title, link, summary, embedding_vector, posted_date, department)
VALUES
    (%s, %s, %s, %s, %s, %s, %s, %s) AS new
ON DUPLICATE KEY UPDATE
    title = new.title,
    link = new.link,
    summary = new.summary,
    embedding_vector = new.embedding_vector,
    posted_date = new.posted_date,
    department = new.department
"""

EXISTS_SQL = "SELECT posted_date FROM notice WHERE category=%s AND post_number=%s LIMIT 1"

def get_existing_posted_date(category: str, post_number: int) -> Optional[str]:
    with mysql_conn() as conn:
        cur = conn.cursor()
        cur.execute(EXISTS_SQL, (category, post_number))
        row = cur.fetchone()
        cur.close()
        return row[0] if row else None

def upsert_notice(row: dict):
    with mysql_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            UPSERT_SQL,
            (
                row["category"],
                row["post_number"],
                row["title"],
                row["link"],
                row.get("summary") or None,
                row.get("embedding_vector") or None,
                row["posted_date"],
                row.get("department") or None,
            ),
        )
        cur.close()

def _ymd(x: Optional[object]) -> Optional[str]:
    if x is None:
        return None
    if isinstance(x, (datetime, date)):
        return x.strftime("%Y-%m-%d")
    s = str(x).strip()
    return s[:10]

# =========================
# 6) íŒŒì´í”„ë¼ì¸: í•œ ê±´ ì²˜ë¦¬ (HTML í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ë™ì‹œ ìš”ì•½)
# =========================
def process_one(category_key: str, list_id: str, seq: int) -> str:
    # 1) HTML
    html = fetch_notice_html(list_id, seq)
    if not html:
        print(f"âš ï¸ Seq {seq}: HTML ë¡œë“œ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
        return "skipped_error"

    # 2) íŒŒì‹±
    parsed = parse_notice_fields(html, seq)
    if not parsed:
        print(f"Seq {seq}: ê²Œì‹œë¬¼ ì—†ìŒ")
        return "not_found"

    post_number = parsed["post_number"]
    title = parsed["title"]
    department = parsed["department"]
    posted_date = parsed["posted_date"]

    # 3) ë§í¬
    crawl_link = f"{CRAWL_VIEW_URL}list_id={list_id}&seq={seq}"
    db_link    = f"{SAVE_VIEW_URL}?{urlencode({'list_id': list_id, 'seq': seq})}"

    # 4) ì¤‘ë³µ ì²´í¬
    prev_dt_raw = get_existing_posted_date(category_key, post_number)
    prev_dt = _ymd(prev_dt_raw)
    curr_dt = _ymd(posted_date)

    if prev_dt:
        if prev_dt == curr_dt:
            # ë‚ ì§œê¹Œì§€ ë™ì¼ â†’ ìŠ¤í‚µ
            print(f"Seq {seq} (post_number={post_number}) ì´ë¯¸ ì¡´ì¬ (posted_date={curr_dt}) â†’ ìŠ¤í‚µ")
            return "stored"
        else:
            # ë‚ ì§œê°€ ë‹¤ë¦„ â†’ ìˆ˜ì •ëœ ê²Œì‹œë¬¼ë¡œ ê°„ì£¼
            print(f"Seq {seq} (post_number={post_number}) ë‚ ì§œ ë³€ê²½ {prev_dt} â†’ {curr_dt}, ì—…ë°ì´íŠ¸ ì§„í–‰")

    # 4-1) HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    html_text = extract_main_text_from_html(html)

    # 5) HTML â†’ ì „ì²´ ì´ë¯¸ì§€ ìº¡ì²˜ (ìŠ¬ë¼ì´ìŠ¤ í¬í•¨)
    imgs = html_to_images_playwright(
        crawl_link,
        viewport_width=1200,
        slice_height=1800,
        debug_full_image_path=None,     # ì „ì²´ 1ì¥ ì €ì¥
        full_image_format="png",
    )
    if not imgs:
        print(f"â†³ Seq {seq}: ì´ë¯¸ì§€ ìº¡ì²˜ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
        return "skipped_error"

    # 6) í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ë™ì‹œ ìš”ì•½
    summary = summarize_with_text_and_images(html_text, imgs)
    if not summary:
        print(f"â†³ Seq {seq}: í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ìš”ì•½ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
        return "skipped_error"

    print(summary)

    # 8) DB ì—…ì„œíŠ¸
    row = {
        "category": category_key,
        "post_number": post_number,
        "title": title,
        "link": db_link,
        "summary": summary,
        "embedding_vector": None,
        "posted_date": posted_date,
        "department": department,
        "viewCount" : "0"
    } 
    try:
        upsert_notice(row)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: [{category_key}] seq={seq}, post_number={post_number}, posted_date={posted_date}, title={title[:30]}...")
        return "stored"
    except MySQLError as e:
        print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e.__class__.__name__}({getattr(e,'errno',None)}): {e}")
        tb = traceback.format_exc(limit=3)
        print(f"â†³ Traceback(ìš”ì•½):\n{tb}")
        return "skipped_error"


# =========================
# 7) ëª©ë¡ HTMLì—ì„œ seq ì¶”ì¶œ
# =========================

def extract_seqs_skip_pinned(html: str) -> List[int]:
    """
    ëª©ë¡ì—ì„œ 'ê³µì§€' ë°°ì§€ê°€ ë¶™ì€ ê³ ì •ê¸€ì„ ì œì™¸í•˜ê³  seqë§Œ ì¶”ì¶œ.
    - ê³ ì •ê¸€ ë§ˆí¬ì—…: <p class="num"><span class="cl">ê³µì§€</span></p>
    - ì¼ë°˜ê¸€: <p class="num">1506</p> ì²˜ëŸ¼ ìˆ«ì í‘œì‹œ
    """
    soup = BeautifulSoup(html, "html.parser")
    seqs: List[int] = []

    # li ë‹¨ìœ„ë¡œ í›‘ë˜, p.num ì•ˆì— span.cl(=ê³µì§€) ìˆìœ¼ë©´ skip
    for li in soup.select("li"):
        num = li.select_one("p.num")
        if num and (num.select_one("span.cl") or "ê³µì§€" in num.get_text(strip=True)):
            continue  # ğŸ”¸ ê³ ì •ê¸€ ìŠ¤í‚µ

        # li ì•ˆì—ì„œ view.do ë§í¬ ì°¾ê³  seq ì¶”ì¶œ
        hrefs = [a.get("href", "") for a in li.select("a[href]")]
        found = False
        for href in hrefs:
            m = re.search(r"(?:\?|&|&amp;)seq=(\d+)", href)
            if m:
                seqs.append(int(m.group(1)))
                found = True
                break
        if found:
            continue

        # hrefì— ì—†ìœ¼ë©´ onclick ê³„ì—´ì—ì„œ ë³´ì¡° ì¶”ì¶œ (ì˜ˆ: goDetail('xxx','15583') or goDetail('xxx',15583))
        txt = li.decode()
        m = re.search(r"\(\s*['\"][^'\"]*['\"]\s*,\s*'(\d+)'\s*\)", txt)
        if not m:
            m = re.search(r"\(\s*['\"][^'\"]*['\"]\s*,\s*(\d+)\s*\)", txt)
        if m:
            seqs.append(int(m.group(1)))

    # ìˆœì„œ ìœ ì§€í•œ ì¤‘ë³µ ì œê±°
    return list(OrderedDict.fromkeys(seqs))


def extract_seqs_from_list_html(html: str) -> List[int]:
    seqs: List[int] = []
    for m in re.finditer(r"view\.do[^\"'>]*(?:\?|&|&amp;)seq=(\d+)", html):
        seqs.append(int(m.group(1)))
    for m in re.finditer(r"\(\s*['\"][^'\"]*['\"]\s*,\s*'(\d+)'\s*\)", html):
        seqs.append(int(m.group(1)))
    for m in re.finditer(r"\(\s*['\"][^'\"]*['\"]\s*,\s*(\d+)\s*\)", html):
        seqs.append(int(m.group(1)))
    return list(OrderedDict.fromkeys(seqs))


def collect_recent_seqs(list_id: str,
                        extra_params: Optional[Dict[str, str]] = None,
                        limit: int = RECENT_WINDOW,
                        max_pages: int = 10) -> List[int]:
    headers = {"User-Agent": "Mozilla/5.0", "Referer": "https://www.uos.ac.kr/"}
    collected: List[int] = []
    seen = set()

    for page in range(1, max_pages + 1):
        params = {"list_id": list_id, "pageIndex": str(page), "searchCnd": "", "searchWrd": ""}
        if extra_params:
            params.update(extra_params)

        r = requests.get(CRAWL_LIST_URL, params=params, headers=headers, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT))
        if r.status_code != 200:
            print(f"âŒ ëª©ë¡ HTTP {r.status_code} (list_id={list_id}, page={page}, params={params})")
            break

        if page == 1:
            page_seqs = extract_seqs_skip_pinned(r.text)
        else:
            page_seqs = extract_seqs_from_list_html(r.text)

        new_count = 0
        for s in page_seqs:
            if s not in seen:
                seen.add(s)
                collected.append(s)
                new_count += 1
                if len(collected) >= limit:
                    return collected

        if new_count == 0:
            break

        time.sleep(0.2)

    return collected

# =========================
# 8-1) í™”í•™ê³µí•™ê³¼
# =========================

def collect_recent_seqs_cheme(limit: int = 100, max_pages: int = 20) -> List[int]:
    headers = {"User-Agent": "Mozilla/5.0"}
    collected: List[int] = []
    seen = set()

    for page in range(1, max_pages + 1):
        params = {"bo_table": "notice", "page": page}
        r = requests.get(CHEME_LIST_URL, params=params, headers=headers, timeout=(10, 20))
        if r.status_code != 200:
            print(f"âŒ í™”í•™ê³µí•™ê³¼ ëª©ë¡ ìš”ì²­ ì‹¤íŒ¨ page={page}: {r.status_code}")
            break

        soup = BeautifulSoup(r.text, "html.parser")

        # wr_id ìˆ˜ì§‘ (ëŒ“ê¸€ ì•µì»¤ ë“± ì œì™¸)
        page_ids: List[int] = []
        for a in soup.select("a[href*='wr_id=']"):
            href = a.get("href", "")
            m = re.search(r"wr_id=(\d+)", href)
            if m:
                wr_id = int(m.group(1))
                # (ì„ íƒ) ëŒ“ê¸€ ì•µì»¤, íŒŒì¼ ë§í¬ ë“± ì œì™¸ ì¡°ê±´ì´ í•„ìš”í•˜ë©´ ì—¬ê¸°ì„œ í•„í„°
                page_ids.append(wr_id)

        # ì¤‘ë³µ ì œê±° + ìˆœì„œ ìœ ì§€
        page_ids = list(OrderedDict.fromkeys(page_ids))

        # ìƒˆë¡œ ë³¸ wr_idë§Œ ì¶”ê°€
        new_cnt = 0
        for wid in page_ids:
            if wid not in seen:
                seen.add(wid)
                collected.append(wid)
                new_cnt += 1
                if len(collected) >= limit:
                    return collected

        # ì´ í˜ì´ì§€ì—ì„œ ìƒˆë¡œ ì–»ì€ ê²Œ ì—†ìœ¼ë©´ ì¤‘ë‹¨
        if new_cnt == 0:
            break

        time.sleep(0.2) 

    return collected

def fetch_notice_html_cheme(wr_id: int) -> Optional[str]:
    """í™”í•™ê³µí•™ê³¼ ê°œë³„ ê³µì§€ HTML ê°€ì ¸ì˜¤ê¸°"""
    url = f"{CHEME_LIST_URL}&wr_id={wr_id}"
    headers = {"User-Agent": "Mozilla/5.0"}
    r = requests.get(url, headers=headers, timeout=(10, 20))
    if r.status_code != 200:
        print(f"âŒ í™”í•™ê³µí•™ê³¼ ìƒì„¸ ìš”ì²­ ì‹¤íŒ¨ wr_id={wr_id}, status={r.status_code}")
        return None
    return r.text

def parse_date_any(text: str) -> Optional[str]:
    if not text:
        return None
    t = text.strip()
    # ì˜ˆ: 25-09-24, 25-09-24 11:02, (25-09-24) ë“± ë³€í˜•ë„ í—ˆìš©
    m = re.search(r'(?<!\d)(?P<yy>\d{2})-(?P<mm>\d{2})-(?P<dd>\d{2})(?!\d)', t)
    if m:
        yy = int(m['yy']); mm = int(m['mm']); dd = int(m['dd'])
        yyyy = 2000 + yy          # 20xxë¡œ í•´ì„
        return f"{yyyy:04d}-{mm:02d}-{dd:02d}"
    return None

def parse_notice_fields_cheme(html: str, wr_id: int) -> Optional[dict]:
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(html, "html.parser")

    # âœ… ì œëª©: h2#bo_v_title > span.bo_v_tit
    title_el = soup.select_one("#bo_v_title .bo_v_tit") or soup.select_one("#bo_v_title")
    title = title_el.get_text(" ", strip=True) if title_el else ""

    # âœ… ë³¸ë¬¸: section#bo_v_atc (gnuboard ë³¸ë¬¸ ì˜ì—­)
    content_el = soup.select_one("#bo_v_atc") or soup.select_one(".board_view, .view_content, #bo_v")
    content_text = content_el.get_text("\n", strip=True) if content_el else soup.get_text("\n", strip=True)

    # âœ… ë‚ ì§œ: section#bo_v_info ë“±
    date_el = soup.select_one("#bo_v_info, .bo_v_info, .view_info, .board_view .info")
    date_text = date_el.get_text(" ", strip=True) if date_el else datetime.now().strftime("%Y-%m-%d")

    # ì¡°íšŒìˆ˜ ì¶”ì¶œ (ì˜ˆì‹œ: 33)
    view_count_el = soup.select_one("strong > i.fa-eye")  # ì¡°íšŒìˆ˜ì— í•´ë‹¹í•˜ëŠ” i íƒœê·¸ë¥¼ ì„ íƒ

    if view_count_el:
        raw_text = view_count_el.find_previous("strong").text.strip()
        m = re.search(r'\d+', raw_text)  # ìˆ«ìë§Œ ì¶”ì¶œ
        view_count = int(m.group()) if m else 0
    else:
        view_count = 0  

    return {
        "title": title,                         # â† ì´ì œ ê¹”ë”í•œ ì œëª©
        "department": "í™”í•™ê³µí•™ê³¼",
        "posted_date": parse_date_any(date_text) or datetime.now().strftime("%Y-%m-%d"),
        "post_number": wr_id,
        "content_text": content_text,
        "view_count": view_count  # ì¡°íšŒìˆ˜ ì¶”ê°€
    }

def process_one_cheme(wr_id: int) -> str:
    """í™”í•™ê³µí•™ê³¼ ê³µì§€ì‚¬í•­ í•œ ê±´ ì²˜ë¦¬ (í¬í„¸ ë°©ì‹ê³¼ ë™ì¼í•˜ê²Œ)"""
    html = fetch_notice_html_cheme(wr_id)
    if not html:
        print(f"âš ï¸ wr_id={wr_id}: HTML ë¡œë“œ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
        return "skipped_error"

    parsed = parse_notice_fields_cheme(html, wr_id)
    if not parsed:
        print(f"wr_id={wr_id}: ê²Œì‹œë¬¼ ì—†ìŒ")
        return "not_found"

    post_number = parsed["post_number"]
    title = parsed["title"]
    department = parsed["department"]
    posted_date = parsed["posted_date"]
    view_count = parsed["view_count"]  

    # ë§í¬
    crawl_link = f"{CHEME_LIST_URL}&wr_id={wr_id}"
    db_link    = crawl_link  # DBì— ì €ì¥í•  ë§í¬

    # ì¤‘ë³µ ì²´í¬
    prev_dt_raw = get_existing_posted_date("COLLEGE_ENGINEERING", post_number)
    prev_dt = _ymd(prev_dt_raw)
    curr_dt = _ymd(posted_date)

    if prev_dt:
        if prev_dt == curr_dt:
            print(f"wr_id={wr_id} (post_number={post_number}) ì´ë¯¸ ì¡´ì¬ (posted_date={curr_dt}) â†’ ìŠ¤í‚µ")
            return "stored"
        else:
            print(f"wr_id={wr_id} (post_number={post_number}) ë‚ ì§œ ë³€ê²½ {prev_dt} â†’ {curr_dt}, ì—…ë°ì´íŠ¸ ì§„í–‰")

    # HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    html_text = extract_main_text_from_html(html)

    # HTML â†’ ì „ì²´ ì´ë¯¸ì§€ ìº¡ì²˜
    imgs = html_to_images_playwright(
        crawl_link,
        viewport_width=1200,
        slice_height=1800,
        debug_full_image_path=None,
        full_image_format="png",
    )
    if not imgs:
        print(f"â†³ wr_id={wr_id}: ì´ë¯¸ì§€ ìº¡ì²˜ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
        return "skipped_error"

    # í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ë™ì‹œ ìš”ì•½
    summary = summarize_with_text_and_images(html_text, imgs)
    if not summary:
        print(f"â†³ wr_id={wr_id}: í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ìš”ì•½ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
        return "skipped_error"

    print(summary)
    # DB ì—…ì„œíŠ¸
    row = {
        "category": "COLLEGE_ENGINEERING",
        "post_number": post_number,
        "title": title,
        "link": db_link,
        "summary": summary,
        "embedding_vector": None,
        "posted_date": posted_date,
        "department": department,
        "view_count": view_count
    }
    try:
        upsert_notice(row)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: [í™”í•™ê³µí•™ê³¼] wr_id={wr_id}, post_number={post_number}, title={title[:50]}, link={db_link}, posted_date={posted_date}, department={department}, viewCount={view_count}")
        return "stored"
    except MySQLError as e:
        print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e.__class__.__name__}({getattr(e,'errno',None)}): {e}")
        tb = traceback.format_exc(limit=3)
        print(f"â†³ Traceback(ìš”ì•½):\n{tb}")
        return "skipped_error"
    
# =========================
# 8-2) ìƒëª…ê³¼í•™ê³¼
# =========================

def collect_recent_seqs_lifesci(limit: int = 100, max_pages: int = 20) -> List[int]:
    headers = {"User-Agent": "Mozilla/5.0"}
    collected: List[int] = []
    seen = set()

    for page in range(1, max_pages + 1):
        params = {"page": page}
        r = requests.get(LIFE_SCI_LIST_URL, params=params, headers=headers, timeout=(10, 20))
        if r.status_code != 200:
            print(f"âŒ ìƒëª…ê³¼í•™ê³¼ ëª©ë¡ ìš”ì²­ ì‹¤íŒ¨ page={page}: {r.status_code}")
            break

        soup = BeautifulSoup(r.text, "html.parser")

        # âœ… ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°ë‚˜ ë°°ì§€ í´ë˜ìŠ¤ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , bbsidx ë§í¬ë§Œ ìˆ˜ì§‘
        page_ids: List[int] = []
        for a in soup.select('a[href*="bbsidx="]'):
            href = a.get("href", "")
            m = re.search(r"bbsidx=(\d+)", href)
            if m:
                page_ids.append(int(m.group(1)))

        # ì¤‘ë³µ ì œê±° + ìˆœì„œ ìœ ì§€
        page_ids = list(OrderedDict.fromkeys(page_ids))

        new_cnt = 0
        for idx in page_ids:
            if idx not in seen:
                seen.add(idx)
                collected.append(idx)
                new_cnt += 1
                if len(collected) >= limit:
                    return collected

        if new_cnt == 0:
            break

        time.sleep(0.2)

    return collected

def fetch_notice_html_lifesci(bbsidx: int) -> Optional[str]:
    """ìƒëª…ê³¼í•™ê³¼ ê°œë³„ ê³µì§€ HTML ê°€ì ¸ì˜¤ê¸° (í™”ê³µê³¼ fetch í•¨ìˆ˜ì™€ êµ¬ì¡° ë™ì¼)"""
    # URL êµ¬ì¡°: ...notice?md=v&bbsidx=11971
    url = f"{LIFE_SCI_LIST_URL}?md=v&bbsidx={bbsidx}"
    headers = {"User-Agent": "Mozilla/5.0"}
    r = requests.get(url, headers=headers, timeout=(10, 20))
    if r.status_code != 200:
        print(f"âŒ ìƒëª…ê³¼í•™ê³¼ ìƒì„¸ ìš”ì²­ ì‹¤íŒ¨ bbsidx={bbsidx}, status={r.status_code}")
        return None
    return r.text

def parse_notice_fields_lifesci(html: str, bbsidx: int) -> Optional[dict]:
    soup = BeautifulSoup(html, "html.parser")

    # âœ… ì œëª©: h1.bbstitle
    title_el = soup.select_one("h1.bbstitle")
    title = title_el.get_text(" ", strip=True) if title_el else ""

    # âœ… ë‚ ì§œ/ì¡°íšŒìˆ˜: div.writer ì•ˆì˜ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
    writer_el = soup.select_one("div.writer")
    date_text, view_count = "", 0
    if writer_el:
        text = writer_el.get_text(" ", strip=True)
        # ë‚ ì§œ ì¶”ì¶œ (ì˜ˆ: 2022-07-15)
        m_date = re.search(r"\d{4}-\d{2}-\d{2}", text)
        if m_date:
            date_text = m_date.group()
        # ì¡°íšŒìˆ˜ ì¶”ì¶œ (ì˜ˆ: ì¡°íšŒìˆ˜ 525)
        m_view = re.search(r"ì¡°íšŒìˆ˜\s*([0-9,]+)", text)
        if m_view:
            view_count = int(m_view.group(1).replace(",", ""))

    # âœ… ë³¸ë¬¸ ì¶”ì¶œ: extract_main_text_from_html ì´ìš©
    content_text = extract_main_text_from_html(html)

    # âœ… ë‚ ì§œ ì—†ìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œë¡œ ëŒ€ì²´
    posted_date = parse_date_yyyy_mm_dd(date_text) or datetime.now().strftime("%Y-%m-%d")

    return {
        "title": title,
        "department": "ìƒëª…ê³¼í•™ê³¼",
        "posted_date": posted_date,
        "post_number": bbsidx,
        "content_text": content_text,
        "view_count": view_count,
    }

def process_one_lifesci(bbsidx: int) -> str:
    """ìƒëª…ê³¼í•™ê³¼ ê³µì§€ì‚¬í•­ í•œ ê±´ ì²˜ë¦¬ (í™”ê³µê³¼ process í•¨ìˆ˜ì™€ êµ¬ì¡° ë™ì¼)"""
    html = fetch_notice_html_lifesci(bbsidx)
    if not html:
        print(f"âš ï¸ bbsidx={bbsidx}: HTML ë¡œë“œ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
        return "skipped_error"

    parsed = parse_notice_fields_lifesci(html, bbsidx)
    if not parsed:
        print(f"bbsidx={bbsidx}: ê²Œì‹œë¬¼ ì—†ìŒ")
        return "not_found"

    post_number = parsed["post_number"]
    title = parsed["title"]
    department = parsed["department"]
    posted_date = parsed["posted_date"]
    view_count = parsed["view_count"] 

    # ë§í¬
    crawl_link = f"{LIFE_SCI_LIST_URL}?md=v&bbsidx={bbsidx}"
    db_link = crawl_link 

    # ì¤‘ë³µ ì²´í¬: ìì—°ê³¼í•™ëŒ€í•™ ì¹´í…Œê³ ë¦¬ ì‚¬ìš© (COLLEGE_NATURAL_SCIENCES)
    prev_dt_raw = get_existing_posted_date("COLLEGE_NATURAL_SCIENCES", post_number)
    prev_dt = _ymd(prev_dt_raw)
    curr_dt = _ymd(posted_date)

    if prev_dt:
        if prev_dt == curr_dt:
            print(f"bbsidx={bbsidx} (post_number={post_number}) ì´ë¯¸ ì¡´ì¬ (posted_date={curr_dt}) â†’ ìŠ¤í‚µ")
            return "stored"
        else:
            print(f"bbsidx={bbsidx} (post_number={post_number}) ë‚ ì§œ ë³€ê²½ {prev_dt} â†’ {curr_dt}, ì—…ë°ì´íŠ¸ ì§„í–‰")

    # HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    html_text = extract_main_text_from_html(html)

    # HTML â†’ ì „ì²´ ì´ë¯¸ì§€ ìº¡ì²˜
    imgs = html_to_images_playwright(
        crawl_link,
        viewport_width=1200,
        slice_height=1800,
        debug_full_image_path=None,
        full_image_format="png",
    )
    if not imgs:
        print(f"â†³ bbsidx={bbsidx}: ì´ë¯¸ì§€ ìº¡ì²˜ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
        return "skipped_error"

    # í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ë™ì‹œ ìš”ì•½
    summary = summarize_with_text_and_images(html_text, imgs)
    if not summary:
        print(f"â†³ bbsidx={bbsidx}: í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ìš”ì•½ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
        return "skipped_error"

    print(summary)
    
    # DB ì—…ì„œíŠ¸
    row = {
        "category": "COLLEGE_NATURAL_SCIENCES",
        "post_number": post_number,
        "title": title,
        "link": db_link,
        "summary": summary,
        "embedding_vector": None,
        "posted_date": posted_date,
        "department": department,
        "view_count": view_count
    }
    try:
        upsert_notice(row)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: [ìƒëª…ê³¼í•™ê³¼] bbsidx={bbsidx}, post_number={post_number}, title={title[:50]}, link={db_link}, posted_date={posted_date}, viewcount={view_count}, department={department}")
        return "stored"
    except MySQLError as e:
        print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e.__class__.__name__}: {e}")
        traceback.print_exc(limit=3, file=sys.stdout)
        return "skipped_error"

# =========================
# 9) ì‹¤í–‰ë¶€
# =========================
def main() -> int:
    print(f"Screenshot directory: {OUT_DIR}")

    targets = [
        "GENERAL",
        "ACADEMIC",
        "COLLEGE_ENGINEERING",
        "COLLEGE_HUMANITIES",
        "COLLEGE_SOCIAL_SCIENCES",
        "COLLEGE_URBAN_SCIENCE",
        "COLLEGE_ARTS_SPORTS",
        "COLLEGE_BUSINESS",
        "COLLEGE_NATURAL_SCIENCES",
        "COLLEGE_LIBERAL_CONVERGENCE"
    ]

    for cat in targets:
        list_id = CATEGORIES.get(cat)
        if not list_id or "TODO" in list_id.lower():
            print(f"â­ï¸  {cat}: list_id ë¯¸ì„¤ì • â†’ ê±´ë„ˆëœ€")
            continue

        seqs = collect_recent_seqs(list_id, extra_params=None, limit=RECENT_WINDOW, max_pages=10)

        if not seqs:
            print(f"âš ï¸ {cat}: ëª©ë¡ì—ì„œ seqë¥¼ ì°¾ì§€ ëª»í•´ ê±´ë„ˆëœ€")
            continue

        print(f"==== [{cat}] list_id={list_id}, {len(seqs)}ê°œ ìˆ˜ì§‘ë¨ (ëª©ë¡ ë…¸ì¶œ í•­ëª©ë§Œ) ====")
        for seq in reversed(seqs):
            process_one(cat, list_id, seq)
            time.sleep(REQUEST_SLEEP)

    # # ğŸ”¹ í™”í•™ê³µí•™ê³¼ ê³µì§€ ì²˜ë¦¬
    seqs = collect_recent_seqs_cheme(limit=100)
    
    print(f"==== [í™”í•™ê³µí•™ê³¼] {len(seqs)}ê°œ ìˆ˜ì§‘ë¨ ====", flush=True)
    for wr_id in reversed(seqs):
        process_one_cheme(wr_id)
        time.sleep(REQUEST_SLEEP)

    # ğŸ”¹ ìƒëª…ê³¼í•™ê³¼ ê³µì§€ ì²˜ë¦¬
    seqs = collect_recent_seqs_lifesci(limit=100)
    
    print(f"==== [ìƒëª…ê³¼í•™ê³¼] {len(seqs)}ê°œ ìˆ˜ì§‘ë¨ ====", flush=True)
    for wr_id in reversed(seqs):
        process_one_lifesci(wr_id)
        time.sleep(REQUEST_SLEEP)

    return 0

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        log(f"ERROR: {type(e).__name__}: {e}")
        traceback.print_exc()
        sys.exit(1)
