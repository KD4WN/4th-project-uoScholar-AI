# src/uosai/crawler/notice_crawler.py

import os
import time
import re
from contextlib import contextmanager
from typing import Optional, Dict, List

import base64
from io import BytesIO
from collections import OrderedDict
from urllib.parse import urlencode

import requests
from bs4 import BeautifulSoup
import mysql.connector
from mysql.connector import Error as MySQLError

from openai import OpenAI
from PIL import Image  # ì´ë¯¸ì§€ ì²˜ë¦¬
from datetime import date, datetime
import sys, traceback

from dotenv import load_dotenv
load_dotenv()

# Playwright
try:
    from playwright.sync_api import sync_playwright
    _PLAYWRIGHT_AVAILABLE = True
except Exception:
    _PLAYWRIGHT_AVAILABLE = False

# =========================
# 0) í™˜ê²½ì„¤ì •
# =========================
BASE_DIR = os.path.abspath(os.getcwd())
OUT_DIR = os.path.join(BASE_DIR, "screenshot")
os.makedirs(OUT_DIR, exist_ok=True)

DB_CONFIG = {
    "host": os.getenv("DB_HOST"),
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASSWORD"),
    "database": os.getenv("DB_NAME"),
    "port": int(os.getenv("DB_PORT", "3306")),
    "charset": os.getenv("DB_CHARSET", "utf8mb4"),
    "autocommit": os.getenv("DB_AUTOCOMMIT", "False") == "True",
    "use_pure": os.getenv("DB_USE_PURE", "True") == "True",
    "connection_timeout": int(os.getenv("DB_CONN_TIMEOUT", 20)),
    "raise_on_warnings": os.getenv("DB_WARNINGS", "True") == "True",
}

# OpenAI
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY").strip()
client = OpenAI(api_key=OPENAI_API_KEY)
SUMMARIZE_MODEL = "gpt-4o"

#################################################################################
# ì¹´í…Œê³ ë¦¬ â†” list_id ë§¤í•‘ (í¬í„¸ ê³µí†µ)
CATEGORIES: Dict[str, str] = {
    "COLLEGE_ENGINEERING": "20013DA1",
    "COLLEGE_HUMANITIES": "human01",
    "COLLEGE_SOCIAL_SCIENCES": "econo01",
    "COLLEGE_URBAN_SCIENCE": "urbansciences01",
    "COLLEGE_ARTS_SPORTS": "artandsport01",
    "COLLEGE_BUSINESS": "20008N2",
    "COLLEGE_NATURAL_SCIENCES": "scien01",
    "COLLEGE_LIBERAL_CONVERGENCE": "clacds01",
    "GENERAL": "FA1",
    "ACADEMIC": "FA2",
}

CRAWL_VIEW_URL = "https://www.uos.ac.kr/korNotice/view.do?identified=anonymous&"
CRAWL_LIST_URL = "https://www.uos.ac.kr/korNotice/list.do?identified=anonymous&"
SAVE_VIEW_URL = "https://www.uos.ac.kr/korNotice/view.do"

#################################################################################
# í•™ê³¼ë³„ ë…ë¦½ URL ì„¤ì • (ê° í•™ê³¼ë§ˆë‹¤ ë‹¤ë¥¸ base_urlê³¼ íŒŒì‹± ë¡œì§)
#################################################################################
DEPT_CONFIGS = {
    "DEPT_CHEMICAL_ENGINEERING": {
        "category": "COLLEGE_ENGINEERING",
        "department": "í™”í•™ê³µí•™ê³¼",
        "list_url": "https://cheme.uos.ac.kr/bbs/board.php?bo_table=notice",
        "id_param": "wr_id",        # URL íŒŒë¼ë¯¸í„°ëª…
        "list_params": {"bo_table": "notice"},  # ëª©ë¡ ì¡°íšŒìš© ê¸°ë³¸ íŒŒë¼ë¯¸í„°
        "url_type": "query",  # query íŒŒë¼ë¯¸í„° ë°©ì‹
        "selectors": {
            "title": ["#bo_v_title .bo_v_tit", "#bo_v_title"],
            "content": ["#bo_v_atc", ".board_view", ".view_content", "#bo_v"],
            "date_info": ["#bo_v_info", ".bo_v_info", ".view_info", ".board_view .info"],
            "view_count": "strong > i.fa-eye",  # ì¡°íšŒìˆ˜ ì•„ì´ì½˜
        }
    },
    "DEPT_LIFE_SCIENCE": {
        "category": "COLLEGE_NATURAL_SCIENCES",
        "department": "ìƒëª…ê³¼í•™ê³¼",
        "list_url": "https://lifesci.uos.ac.kr/community/notice",
        "id_param": "bbsidx",
        "list_params": {},  # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì—†ìŒ
        "url_type": "query",  # query íŒŒë¼ë¯¸í„° ë°©ì‹
        "selectors": {
            "title": ["h1.bbstitle"],
            "content": [],  # extract_main_text_from_html ì‚¬ìš©
            "date_info": ["div.writer"],
            "view_count": "div.writer",  # í…ìŠ¤íŠ¸ì—ì„œ íŒŒì‹±
        }
    },
    "DEPT_ECONOMICS": {
        "category": "COLLEGE_SOCIAL_SCIENCES",
        "department": "ê²½ì œí•™ë¶€",
        "list_url": "https://econ.uos.ac.kr/notices/undergraduate",
        "id_param": "post_id",  # ë”ë¯¸ (ì‹¤ì œë¡œëŠ” ê²½ë¡œì—ì„œ ì¶”ì¶œ)
        "list_params": {},
        "url_type": "path",  # ê²½ë¡œ(path) ë°©ì‹ (ì˜ˆ: /notices/19184)
        "detail_url_template": "https://econ.uos.ac.kr/notices/{post_id}",  # ìƒì„¸ URL í…œí”Œë¦¿
        "selectors": {
            "title": ["h2.uos-post-header__title", ".uos-post-header__title"],
            "content": ["div.uos-post__content", ".uos-post__content"],
            "date_info": ["span.uos-meta-section__date-value", ".uos-meta-section__date-value"],
            "view_count": None,  # ì¡°íšŒìˆ˜ ì—†ìŒ
        }
    },
    "DEPT_ARCHITECTURE": {
        "category": "COLLEGE_URBAN_SCIENCE",
        "department": "ê±´ì¶•í•™ë¶€",
        "list_url": "https://uosarch.ac.kr/board/notice/",
        "id_param": "post_slug",  # WordPress slug ë°©ì‹
        "list_params": {},
        "url_type": "slug",  # slug ë°©ì‹ (WordPress)
        "detail_url_base": "https://uosarch.ac.kr/uosarch_notice/",  # ìƒì„¸ URL ë² ì´ìŠ¤
        "selectors": {
            "title": ["h2.__post-title", ".__post-title"],
            "content": ["div.__post-content", ".__post-content"],
            "date_info": ["div.__post-date", ".__post-date", "div.__post-meta"],
            "view_count": "div.__post-view",  # Views 168 í˜•íƒœ
        }
    },
    "DEPT_LANDSCAPE_ARCHITECTURE": {
        "category": "COLLEGE_URBAN_SCIENCE",
        "department": "ì¡°ê²½í•™ê³¼",
        "list_url": "https://lauos.or.kr/notice",
        "id_param": "uid",  # URL íŒŒë¼ë¯¸í„°ëª…
        "list_params": {},
        "url_type": "query",  # query íŒŒë¼ë¯¸í„° ë°©ì‹
        "detail_url_template": "https://lauos.or.kr/notice?uid={post_id}&mod=document",
        "selectors": {
            "title": ["h1"],  # h1 íƒœê·¸ì—ì„œ ì œëª© ì¶”ì¶œ (2ë²ˆì§¸ h1)
            "content": ["div.kboard-content", "div.content-view"],
            "date_info": ["div.detail-value"],  # ì‘ì„±ì¼/ì¡°íšŒìˆ˜ ì •ë³´
            "view_count": "div.detail-value",  # í…ìŠ¤íŠ¸ì—ì„œ íŒŒì‹±
        }
    },
}
#################################################################################

# ëª‡ ê°œ í¬ë¡¤ë§í•  ê±´ì§€ 
REQUEST_SLEEP = 1.0
PLAYWRIGHT_TIMEOUT_MS = 90000
RECENT_WINDOW = 50

# =========================
# 1) ìœ í‹¸
# =========================

# ë¡œê·¸ ì¶œë ¥
def log(msg: str) -> None:
    print(f"[indexer {datetime.now():%Y-%m-%d %H:%M:%S}] {msg}")

@contextmanager
def mysql_conn():
    conn = mysql.connector.connect(**DB_CONFIG)
    try:
        yield conn
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()


def parse_date_yyyy_mm_dd(text: str) -> Optional[str]:
    m = re.search(r"(\d{4}-\d{2}-\d{2})", text or "")
    return m.group(1) if m else None


def extract_main_text_from_html(html: str, max_chars: int = 12000) -> str:
    """
    ê³µì§€ì˜ 'ë³¸ë¬¸' ì»¨í…Œì´ë„ˆì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ.
    ì‚¬ì´ë“œ/í‘¸í„°/ê´€ë ¨ê¸€/ì£¼ì†Œ/ì¹´í”¼ë¼ì´íŠ¸ ë“±ì€ ì œê±°í•˜ê³ ,
    ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ë©´ max_charsë¡œ ì˜ë¼ ëª¨ë¸ ì…ë ¥ì„ ì•ˆì •í™”.
    """
    soup = BeautifulSoup(html, "html.parser")

    # ë³¸ë¬¸ í›„ë³´ ì…€ë ‰í„° (ì‚¬ì´íŠ¸ ë§ê²Œ í•„ìš”ì‹œ ì¶”ê°€)
    candidates = [
        "div.vw-cnt", "div.vw-con", "div.vw-bd", "div.board-view",
        "article", "div#content", "div#contents", "main"
    ]
    main = None
    for sel in candidates:
        node = soup.select_one(sel)
        if node and node.get_text(strip=True):
            main = node
            break
    if main is None:
        main = soup.body or soup

    # ë¶ˆí•„ìš” ì˜ì—­ ì œê±°
    kill_selectors = [
        ".related", ".relate", ".attach", ".file", ".files",
        ".prev", ".next", "footer", "#footer", ".sns", ".share",
        ".copyright", ".copy", ".address", ".addr"
    ]
    for ks in kill_selectors:
        for n in main.select(ks):
            n.decompose()

    text = main.get_text("\n", strip=True)

    # í”í•œ í‘¸í„°/ì£¼ì†Œ/ì¹´í”¼ë¼ì´íŠ¸ ë¬¸êµ¬ ì œê±°
    drop_patterns = [
        r"ì„œìš¸ì‹œë¦½ëŒ€í•™êµ\s*.+?\d{2,3}-\d{3,4}-\d{4}",
        r"Copyright.+?All rights reserved\.?",
        r"ì´ì „ê¸€.*", r"ë‹¤ìŒê¸€.*", r"ê´€ë ¨\s?ê²Œì‹œë¬¼.*",
    ]
    for pat in drop_patterns:
        text = re.sub(pat, "", text, flags=re.I | re.S)

    # ê³µë°± ì •ë¦¬
    text = re.sub(r"\n{3,}", "\n\n", text).strip()

    # ê³¼ë„í•œ ê¸¸ì´ ì œí•œ
    if len(text) > max_chars:
        text = text[:max_chars] + "\n\n[... ë³¸ë¬¸ ì¼ë¶€ ìƒëµ ...]"

    return text


# =========================
# 2) Playwrightë¡œ HTML â†’ ì´ë¯¸ì§€ ìº¡ì²˜
# =========================
def html_to_images_playwright(
    url: str,
    viewport_width: int = 1200,
    slice_height: int = 1920,
    timeout_ms: int = PLAYWRIGHT_TIMEOUT_MS,
    debug_full_image_path: Optional[str] = None,  # ì „ì²´ í˜ì´ì§€ 1ì¥ ì €ì¥ ê²½ë¡œ
    full_image_format: str = "png",               # "png"|"jpeg"
) -> List[Image.Image]:
    """
    í˜ì´ì§€ ì „ì²´ë¥¼ full_page ìŠ¤í¬ë¦°ìƒ·ìœ¼ë¡œ ì°ì€ ë’¤,
    slice_height ê°„ê²©ìœ¼ë¡œ ëê¹Œì§€ ì „ë¶€ ì˜ë¼ì„œ ë°˜í™˜.
    (max_slices ì œí•œ ì—†ìŒ)
    debug_full_image_pathê°€ ì£¼ì–´ì§€ë©´ ì „ì²´ ìŠ¤í¬ë¦°ìƒ· ì›ë³¸ì„ íŒŒì¼ë¡œ ì €ì¥.
    """
    if not _PLAYWRIGHT_AVAILABLE:
        print("âŒ Playwright ë¯¸ì„¤ì¹˜/ì„í¬íŠ¸ ì‹¤íŒ¨")
        return []

    imgs: List[Image.Image] = []
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,
                args=[
                    "--disable-web-security",
                    "--hide-scrollbars",
                    "--disable-blink-features=AutomationControlled",
                ]
            )
            # User-Agent ë° ê¸°íƒ€ í—¤ë” ì„¤ì •
            page = browser.new_page(
                viewport={"width": viewport_width, "height": slice_height},
                device_scale_factor=2.0,
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                extra_http_headers={
                    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                }
            )
            page.goto(url, wait_until="networkidle", timeout=timeout_ms)

            try:
                page.wait_for_selector("div.vw-tibx", timeout=timeout_ms)
            except Exception:
                pass

            for _ in range(6):
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                page.wait_for_timeout(700)

            page.wait_for_load_state("domcontentloaded")
            page.wait_for_timeout(500)

            # ì „ì²´ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ·
            if full_image_format.lower() == "png":
                buf = page.screenshot(full_page=True, type="png")
            else:
                buf = page.screenshot(full_page=True, type="jpeg", quality=85)
            browser.close()

        # ì „ì²´ í˜ì´ì§€ í•œ ì¥ ì €ì¥(í…ŒìŠ¤íŠ¸/ë””ë²„ê·¸)
        if debug_full_image_path:
            try:
                with open(debug_full_image_path, "wb") as f:
                    f.write(buf)
                print(f"ğŸ’¾ Full screenshot saved: {debug_full_image_path}")
            except Exception as e:
                print(f"âš ï¸ Full screenshot save failed: {e}")

        # ìŠ¬ë¼ì´ìŠ¤ ë¶„í• 
        full_img = Image.open(BytesIO(buf)).convert("RGB")
        W, H = full_img.size
        y = 0
        while y < H:
            crop = full_img.crop((0, y, W, min(y + slice_height, H)))
            imgs.append(crop)
            y += slice_height

    except Exception as e:
        print(f"âŒ HTMLâ†’ì´ë¯¸ì§€ ìº¡ì²˜ ì‹¤íŒ¨: {e}")

    return imgs


# =========================
# 3) OpenAI: ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ìš”ì•½ + ì„ë² ë”©
# =========================
def pil_to_data_url(pil_image: Image.Image, fmt="JPEG", quality=80) -> str:
    bio = BytesIO()
    pil_image.save(bio, format=fmt, quality=quality, optimize=True)
    b64 = base64.b64encode(bio.getvalue()).decode("utf-8")

    return f"data:image/{fmt.lower()};base64,{b64}"

def summarize_with_text_and_images(html_text: str, images: List[Image.Image]) -> str:
    """
    HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ìš°ì„  ê·¼ê±°ë¡œ ì‚¼ê³ ,
    ì´ë¯¸ì§€(í¬ìŠ¤í„°/í‘œ ë“±)ì—ë§Œ ìˆëŠ” ëˆ„ë½ ì •ë³´ë¥¼ ë³´ê°•í•˜ë„ë¡ ì§€ì‹œ.
    """
    merge_prompt = f"""
ì•„ë˜ëŠ” ëŒ€í•™ ê³µì§€ì‚¬í•­ì˜ 'HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸'ì…ë‹ˆë‹¤. ì´ í…ìŠ¤íŠ¸ë¥¼ **ìš°ì„  ê·¼ê±°**ë¡œ ì‚¼ê³ ,
ì¶”ê°€ë¡œ ì œê³µë˜ëŠ” 'í˜ì´ì§€ ì „ì²´ ìº¡ì²˜ ì´ë¯¸ì§€ë“¤'ì—ì„œë§Œ ë³´ì´ëŠ” í‘œ/í¬ìŠ¤í„°/ìŠ¤ìº”ëœ ë¬¸ì¥ ë“± ëˆ„ë½ ì •ë³´ë¥¼ **ë³´ì™„**í•˜ì—¬
ë‚´ìš©ì„ ë§ë¶™ì—¬ì£¼ì„¸ìš”.

- ë³¸ë¬¸ê³¼ ë¬´ê´€í•œ ì‚¬ì´ë“œ/í‘¸í„°/ì£¼ì†Œ/ì¹´í”¼ë¼ì´íŠ¸/ê´€ë ¨ ê²Œì‹œë¬¼ ë“±ì€ ì œì™¸í•˜ì„¸ìš”.
- ìˆ˜ì¹˜ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ë³´ì¡´
- ë‚ ì§œ ë° ì‹œê°„ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë³´ì¡´
- ê¸°ê´€/ë¶€ì„œ, ì¥ì†Œ, ì „í™”, ë©”ì¼ì€ ì›ë¬¸ í‘œê¸° ê·¸ëŒ€ë¡œ ì‚¬ìš©(ì¶”ì¸¡ ê¸ˆì§€) 
- "ì œê³µëœ HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ì¶”ê°€ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ê³µì§€ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:" ì™€ ê°™ì€, ê³µì§€ ì‚¬í•­ì˜ ë‚´ìš© ì´ì™¸ì˜ ë‹¤ë¥¸ ë©˜íŠ¸ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ë©´ ì•ˆë¨. ì •í™•íˆ ê³µì§€ì‚¬í•­ ë‚´ìš©'ë§Œ' í¬í•¨í•´ì•¼í•¨.

[HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì‹œì‘]
{html_text}
[HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸ ë]
""".strip()

    contents = [{"type": "input_text", "text": merge_prompt}]
    for img in images:
        contents.append({
            "type": "input_image",
            "image_url": pil_to_data_url(img, fmt="JPEG", quality=75)  # JPEGë¡œ ì••ì¶•
        })
    try:
        resp = client.responses.create(
            model=SUMMARIZE_MODEL,   # "gpt-4o" ê¶Œì¥
            input=[{"role": "user", "content": contents}],
            temperature=0.2,
        )
        return (resp.output_text or "").strip()
    except Exception as e:
        print(f"âŒ í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ìš”ì•½ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        traceback.print_exc(limit=2, file=sys.stdout)
        return ""

# =========================
# 4) HTML íŒŒì‹± (ìƒì„¸)
# =========================
CONNECT_TIMEOUT = 10    # ì„œë²„ TCP ì—°ê²°ê¹Œì§€ ê¸°ë‹¤ë¦´ ìµœëŒ€ ì‹œê°„
READ_TIMEOUT    = 20   # ì‹¤ì œ ì‘ë‹µ(HTML)ì„ ë°›ëŠ” ì‹œê°„

def fetch_notice_html(list_id: str, seq: int) -> Optional[str]:
    try:
        params = {
            "list_id": list_id,
            "seq": str(seq),
            "sort": "1",
            "pageIndex": "1",
            "searchCnd": "",
            "searchWrd": "",
            "cate_id": "",
            "viewAuth": "Y",
            "writeAuth": "Y",
            "board_list_num": "10",
            "lpageCount": "12",
            "menuid": "",
        }
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(CRAWL_VIEW_URL, params=params, headers=headers, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT))
        if r.status_code != 200:
            print(f"âŒ HTTP {r.status_code} for seq={seq}")
            return None
        return r.text
    except Exception as e:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨ seq={seq}: {e}")
        return None


def parse_notice_fields(html: str, seq: int) -> Optional[dict]:
    soup = BeautifulSoup(html, "html.parser")
    title_el = soup.select_one("div.vw-tibx h4") if soup else None
    title = title_el.get_text(strip=True) if title_el else ""
    if not title:
        return None  # ê²Œì‹œë¬¼ ì—†ìŒ

    spans = soup.select("div.vw-tibx div.zl-bx div.da span")
    department = spans[1].get_text(strip=True) if len(spans) >= 3 else ""
    date_text = spans[2].get_text(strip=True) if len(spans) >= 3 else ""
    dt = parse_date_yyyy_mm_dd(date_text) or datetime.now().strftime("%Y-%m-%d")

    post_number_el = soup.select_one("input[name=seq]")
    post_number = int(post_number_el["value"]) if post_number_el and post_number_el.get("value") else int(seq)

    content_text = soup.get_text("\n", strip=True)
    return {
        "title": title,
        "department": department,
        "posted_date": dt,
        "post_number": post_number,
        "content_text": content_text,
    }


# =========================
# 5) DB ì—…ì„œíŠ¸
# =========================
UPSERT_SQL = """
INSERT INTO notice
    (category, post_number, title, link, summary, embedding_vector, posted_date, department, view_count)
VALUES
    (%s, %s, %s, %s, %s, %s, %s, %s, %s) AS new
ON DUPLICATE KEY UPDATE
    title = new.title,
    link = new.link,
    summary = new.summary,
    embedding_vector = new.embedding_vector,
    posted_date = new.posted_date,
    department = new.department,
    view_count = new.view_count
"""

EXISTS_SQL = "SELECT posted_date FROM notice WHERE category=%s AND post_number=%s LIMIT 1"

def get_existing_posted_date(category: str, post_number) -> Optional[str]:
    """
    post_numberëŠ” int ë˜ëŠ” str(slug)ì¼ ìˆ˜ ìˆìŒ
    str(slug)ì¸ ê²½ìš° í•´ì‹œê°’ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
    """
    # post_numberë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜ (slugëŠ” í•´ì‹œê°’ìœ¼ë¡œ)
    post_num = _normalize_post_number(post_number)

    with mysql_conn() as conn:
        cur = conn.cursor()
        cur.execute(EXISTS_SQL, (category, post_num))
        row = cur.fetchone()
        cur.close()
        return row[0] if row else None


def _normalize_post_number(post_number) -> int:
    """
    post_numberë¥¼ ì •ìˆ˜ë¡œ ì •ê·œí™”
    - ì´ë¯¸ intë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    - str(slug)ì´ë©´ 32ë¹„íŠ¸ í•´ì‹œê°’ìœ¼ë¡œ ë³€í™˜
    """
    if isinstance(post_number, int):
        return post_number

    # ë¬¸ìì—´ì¸ ê²½ìš° CRC32 í•´ì‹œ ì‚¬ìš© (ì–‘ìˆ˜ ë³´ì¥)
    import zlib
    hash_val = zlib.crc32(post_number.encode('utf-8')) & 0x7fffffff
    return hash_val

def upsert_notice(row: dict):
    with mysql_conn() as conn:
        cur = conn.cursor()
        cur.execute(
            UPSERT_SQL,
            (
                row["category"],
                row["post_number"],
                row["title"],
                row["link"],
                row.get("summary") or None,
                row.get("embedding_vector") or None,
                row["posted_date"],
                row.get("department") or None,
                row.get("view_count") or 0,
            ),
        )
        cur.close()

def _ymd(x: Optional[object]) -> Optional[str]:
    if x is None:
        return None
    if isinstance(x, (datetime, date)):
        return x.strftime("%Y-%m-%d")
    s = str(x).strip()
    return s[:10]

# =========================
# 6) íŒŒì´í”„ë¼ì¸: í•œ ê±´ ì²˜ë¦¬ (HTML í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ë™ì‹œ ìš”ì•½)
# =========================
def process_one(category_key: str, list_id: str, seq: int) -> str:
    # 1) HTML
    html = fetch_notice_html(list_id, seq)
    if not html:
        print(f"âš ï¸ Seq {seq}: HTML ë¡œë“œ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
        return "skipped_error"

    # 2) íŒŒì‹±
    parsed = parse_notice_fields(html, seq)
    if not parsed:
        print(f"Seq {seq}: ê²Œì‹œë¬¼ ì—†ìŒ")
        return "not_found"

    post_number = parsed["post_number"]
    title = parsed["title"]
    department = parsed["department"]
    posted_date = parsed["posted_date"]

    # 3) ë§í¬
    crawl_link = f"{CRAWL_VIEW_URL}list_id={list_id}&seq={seq}"
    db_link    = f"{SAVE_VIEW_URL}?{urlencode({'list_id': list_id, 'seq': seq})}"

    # 4) ì¤‘ë³µ ì²´í¬
    prev_dt_raw = get_existing_posted_date(category_key, post_number)
    prev_dt = _ymd(prev_dt_raw)
    curr_dt = _ymd(posted_date)

    if prev_dt:
        if prev_dt == curr_dt:
            # ë‚ ì§œê¹Œì§€ ë™ì¼ â†’ ìŠ¤í‚µ
            print(f"Seq {seq} (post_number={post_number}) ì´ë¯¸ ì¡´ì¬ (posted_date={curr_dt}) â†’ ìŠ¤í‚µ")
            return "stored"
        else:
            # ë‚ ì§œê°€ ë‹¤ë¦„ â†’ ìˆ˜ì •ëœ ê²Œì‹œë¬¼ë¡œ ê°„ì£¼
            print(f"Seq {seq} (post_number={post_number}) ë‚ ì§œ ë³€ê²½ {prev_dt} â†’ {curr_dt}, ì—…ë°ì´íŠ¸ ì§„í–‰")

    # 4-1) HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    html_text = extract_main_text_from_html(html)

    # 5) HTML â†’ ì „ì²´ ì´ë¯¸ì§€ ìº¡ì²˜ (ìŠ¬ë¼ì´ìŠ¤ í¬í•¨)
    imgs = html_to_images_playwright(
        crawl_link,
        viewport_width=1200,
        slice_height=1800,
        debug_full_image_path=None,     # ì „ì²´ 1ì¥ ì €ì¥
        full_image_format="png",
    )
    if not imgs:
        print(f"â†³ Seq {seq}: ì´ë¯¸ì§€ ìº¡ì²˜ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
        return "skipped_error"

    # 6) í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ë™ì‹œ ìš”ì•½
    summary = summarize_with_text_and_images(html_text, imgs)
    if not summary:
        print(f"â†³ Seq {seq}: í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ìš”ì•½ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
        return "skipped_error"

    print(summary)

    # 8) DB ì—…ì„œíŠ¸
    row = {
        "category": category_key,
        "post_number": post_number,
        "title": title,
        "link": db_link,
        "summary": summary,
        "embedding_vector": None,
        "posted_date": posted_date,
        "department": department,
        "viewCount" : "0"
    } 
    try:
        upsert_notice(row)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: [{category_key}] seq={seq}, post_number={post_number}, posted_date={posted_date}, title={title[:30]}...")
        return "stored"
    except MySQLError as e:
        print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e.__class__.__name__}({getattr(e,'errno',None)}): {e}")
        tb = traceback.format_exc(limit=3)
        print(f"â†³ Traceback(ìš”ì•½):\n{tb}")
        return "skipped_error"


# =========================
# 7) ëª©ë¡ HTMLì—ì„œ seq ì¶”ì¶œ
# =========================

def extract_seqs_skip_pinned(html: str) -> List[int]:
    """
    ëª©ë¡ì—ì„œ 'ê³µì§€' ë°°ì§€ê°€ ë¶™ì€ ê³ ì •ê¸€ì„ ì œì™¸í•˜ê³  seqë§Œ ì¶”ì¶œ.
    - ê³ ì •ê¸€ ë§ˆí¬ì—…: <p class="num"><span class="cl">ê³µì§€</span></p>
    - ì¼ë°˜ê¸€: <p class="num">1506</p> ì²˜ëŸ¼ ìˆ«ì í‘œì‹œ
    """
    soup = BeautifulSoup(html, "html.parser")
    seqs: List[int] = []

    # li ë‹¨ìœ„ë¡œ í›‘ë˜, p.num ì•ˆì— span.cl(=ê³µì§€) ìˆìœ¼ë©´ skip
    for li in soup.select("li"):
        num = li.select_one("p.num")
        if num and (num.select_one("span.cl") or "ê³µì§€" in num.get_text(strip=True)):
            continue  # ğŸ”¸ ê³ ì •ê¸€ ìŠ¤í‚µ

        # li ì•ˆì—ì„œ view.do ë§í¬ ì°¾ê³  seq ì¶”ì¶œ
        hrefs = [a.get("href", "") for a in li.select("a[href]")]
        found = False
        for href in hrefs:
            m = re.search(r"(?:\?|&|&amp;)seq=(\d+)", href)
            if m:
                seqs.append(int(m.group(1)))
                found = True
                break
        if found:
            continue

        # hrefì— ì—†ìœ¼ë©´ onclick ê³„ì—´ì—ì„œ ë³´ì¡° ì¶”ì¶œ (ì˜ˆ: goDetail('xxx','15583') or goDetail('xxx',15583))
        txt = li.decode()
        m = re.search(r"\(\s*['\"][^'\"]*['\"]\s*,\s*'(\d+)'\s*\)", txt)
        if not m:
            m = re.search(r"\(\s*['\"][^'\"]*['\"]\s*,\s*(\d+)\s*\)", txt)
        if m:
            seqs.append(int(m.group(1)))

    # ìˆœì„œ ìœ ì§€í•œ ì¤‘ë³µ ì œê±°
    return list(OrderedDict.fromkeys(seqs))


def extract_seqs_from_list_html(html: str) -> List[int]:
    seqs: List[int] = []
    for m in re.finditer(r"view\.do[^\"'>]*(?:\?|&|&amp;)seq=(\d+)", html):
        seqs.append(int(m.group(1)))
    for m in re.finditer(r"\(\s*['\"][^'\"]*['\"]\s*,\s*'(\d+)'\s*\)", html):
        seqs.append(int(m.group(1)))
    for m in re.finditer(r"\(\s*['\"][^'\"]*['\"]\s*,\s*(\d+)\s*\)", html):
        seqs.append(int(m.group(1)))
    return list(OrderedDict.fromkeys(seqs))


def collect_recent_seqs(list_id: str,
                        extra_params: Optional[Dict[str, str]] = None,
                        limit: int = RECENT_WINDOW,
                        max_pages: int = 10) -> List[int]:
    headers = {"User-Agent": "Mozilla/5.0", "Referer": "https://www.uos.ac.kr/"}
    collected: List[int] = []
    seen = set()

    for page in range(1, max_pages + 1):
        params = {"list_id": list_id, "pageIndex": str(page), "searchCnd": "", "searchWrd": ""}
        if extra_params:
            params.update(extra_params)

        r = requests.get(CRAWL_LIST_URL, params=params, headers=headers, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT))
        if r.status_code != 200:
            print(f"âŒ ëª©ë¡ HTTP {r.status_code} (list_id={list_id}, page={page}, params={params})")
            break

        if page == 1:
            page_seqs = extract_seqs_skip_pinned(r.text)
        else:
            page_seqs = extract_seqs_from_list_html(r.text)

        new_count = 0
        for s in page_seqs:
            if s not in seen:
                seen.add(s)
                collected.append(s)
                new_count += 1
                if len(collected) >= limit:
                    return collected

        if new_count == 0:
            break

        time.sleep(0.2)

    return collected

# =========================
# 8) í•™ê³¼ë³„ í†µí•© ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# =========================

def _collect_arch_slugs_with_rest_api(base_url: str, limit: int = 100) -> List[str]:
    """
    ê±´ì¶•í•™ë¶€ ëª©ë¡ì„ WordPress REST APIë¡œ ìˆ˜ì§‘
    (Playwright ì—†ì´ ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ ë™ì‘)
    """
    # WordPress REST API ì—”ë“œí¬ì¸íŠ¸
    api_url = "https://uosarch.ac.kr/wp-json/wp/v2/uosarch_notice"

    collected = []
    per_page = 100  # í•œ ë²ˆì— ìµœëŒ€ 100ê°œ
    page = 1

    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        while len(collected) < limit:
            params = {
                "per_page": min(per_page, limit - len(collected)),
                "page": page,
                "order": "desc",
                "orderby": "date"
            }

            print(f"[ê±´ì¶•í•™ë¶€] REST API í˜ì´ì§€ {page} ìš”ì²­ ì¤‘...")
            r = requests.get(api_url, params=params, headers=headers, timeout=10)

            if r.status_code != 200:
                print(f"[ê±´ì¶•í•™ë¶€] API ìš”ì²­ ì‹¤íŒ¨: HTTP {r.status_code}")
                break

            posts = r.json()

            if not posts:
                print(f"[ê±´ì¶•í•™ë¶€] í˜ì´ì§€ {page}: ë” ì´ìƒ ê²Œì‹œë¬¼ ì—†ìŒ")
                break

            # slug ì¶”ì¶œ
            for post in posts:
                slug = post.get("slug", "")
                if slug:
                    # URL ë””ì½”ë”© (REST APIëŠ” URL ì¸ì½”ë”©ëœ slug ë°˜í™˜)
                    from urllib.parse import unquote
                    slug = unquote(slug)
                    collected.append(slug)

            print(f"[ê±´ì¶•í•™ë¶€] í˜ì´ì§€ {page}: {len(posts)}ê°œ ìˆ˜ì§‘ (ëˆ„ì : {len(collected)}ê°œ)")

            if len(collected) >= limit:
                break

            # ë‹¤ìŒ í˜ì´ì§€ë¡œ
            page += 1
            time.sleep(0.2)  # API ë¶€í•˜ ë°©ì§€

    except Exception as e:
        print(f"âŒ [ê±´ì¶•í•™ë¶€] REST API í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        traceback.print_exc()

    # limitê¹Œì§€ë§Œ ìë¥´ê¸°
    collected = collected[:limit]
    print(f"[ê±´ì¶•í•™ë¶€] ì´ {len(collected)}ê°œ slug ìˆ˜ì§‘ ì™„ë£Œ")
    return collected


def collect_recent_seqs_generic(dept_key: str, limit: int = 100, max_pages: int = 20) -> List:
    """í•™ê³¼ë³„ ë…ë¦½ URLì—ì„œ ê²Œì‹œë¬¼ ID/slug ìˆ˜ì§‘ (í†µí•©)"""
    config = DEPT_CONFIGS.get(dept_key)
    if not config:
        print(f"âŒ ì„¤ì •ë˜ì§€ ì•Šì€ í•™ê³¼: {dept_key}")
        return []

    list_url = config["list_url"]
    id_param = config["id_param"]
    list_params = config.get("list_params", {})
    url_type = config.get("url_type", "query")  # "query", "path", "slug"

    # ê±´ì¶•í•™ë¶€ëŠ” WordPress REST API ì‚¬ìš©
    if dept_key == "DEPT_ARCHITECTURE":
        return _collect_arch_slugs_with_rest_api(list_url, limit)

    headers = {"User-Agent": "Mozilla/5.0"}
    collected = []
    seen = set()

    for page in range(1, max_pages + 1):
        # URL êµ¬ì„± (url_typeì— ë”°ë¼ ë‹¤ë¦„)
        if url_type in ["path", "slug"]:
            # ê²½ë¡œ/slug ë°©ì‹: /notices/undergraduate/page/2/ or /board/notice/page/2/
            if page == 1:
                url = list_url
            else:
                url = f"{list_url.rstrip('/')}/page/{page}/"
        else:
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ë°©ì‹: ?page=2&bo_table=notice
            params = {"page": page}
            params.update(list_params)
            url = list_url

        # ìš”ì²­
        if url_type in ["path", "slug"]:
            r = requests.get(url, headers=headers, timeout=(10, 20))
        else:
            r = requests.get(url, params=params, headers=headers, timeout=(10, 20))

        if r.status_code != 200:
            print(f"âŒ {dept_key} ëª©ë¡ ìš”ì²­ ì‹¤íŒ¨ page={page}: {r.status_code}")
            break

        soup = BeautifulSoup(r.text, "html.parser")

        # ID/slug ì¶”ì¶œ (url_typeì— ë”°ë¼ ë‹¤ë¦„)
        page_items = []

        if url_type == "slug":
            # WordPress slug ë°©ì‹: /uosarch_notice/slug-name/
            for a in soup.select('a[href*="/uosarch_notice/"]'):
                href = a.get("href", "")
                m = re.search(r"/uosarch_notice/([^/]+)/?$", href)
                if m:
                    slug = m.group(1)
                    # URL ì¸ì½”ë”©ëœ slug ë””ì½”ë”©
                    from urllib.parse import unquote
                    slug = unquote(slug)
                    page_items.append(slug)
        elif url_type == "path":
            # ê²½ë¡œ ë°©ì‹: /notices/19184 ê°™ì€ ë§í¬ì—ì„œ ID ì¶”ì¶œ
            for a in soup.select('a[href*="/notices/"]'):
                href = a.get("href", "")
                m = re.search(r"/notices/(\d+)", href)
                if m:
                    page_items.append(int(m.group(1)))
        else:
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ë°©ì‹: ?wr_id=123 ê°™ì€ ë§í¬ì—ì„œ ID ì¶”ì¶œ
            for a in soup.select(f'a[href*="{id_param}="]'):
                href = a.get("href", "")
                m = re.search(rf"{id_param}=(\d+)", href)
                if m:
                    page_items.append(int(m.group(1)))

        # ì¤‘ë³µ ì œê±° + ìˆœì„œ ìœ ì§€
        page_items = list(OrderedDict.fromkeys(page_items))

        new_cnt = 0
        for item in page_items:
            if item not in seen:
                seen.add(item)
                collected.append(item)
                new_cnt += 1
                if len(collected) >= limit:
                    return collected

        if new_cnt == 0:
            break

        time.sleep(0.2)

    return collected


def fetch_notice_html_generic(dept_key: str, post_id) -> Optional[str]:
    """í•™ê³¼ë³„ ë…ë¦½ URLì—ì„œ ê°œë³„ ê³µì§€ HTML ê°€ì ¸ì˜¤ê¸° (í†µí•©)
    post_idëŠ” int ë˜ëŠ” str(slug)ì¼ ìˆ˜ ìˆìŒ
    """
    config = DEPT_CONFIGS.get(dept_key)
    if not config:
        return None

    url_type = config.get("url_type", "query")

    # URL êµ¬ì„± (url_typeì— ë”°ë¼ ë‹¤ë¦„)
    if url_type == "slug":
        # WordPress slug ë°©ì‹: https://uosarch.ac.kr/uosarch_notice/slug-name/
        from urllib.parse import quote
        detail_url_base = config.get("detail_url_base")
        slug = post_id if isinstance(post_id, str) else str(post_id)
        # slugê°€ ì´ë¯¸ ì¸ì½”ë”©ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        url = f"{detail_url_base}{slug}/"
    elif url_type == "path":
        # ê²½ë¡œ ë°©ì‹: https://econ.uos.ac.kr/notices/19184
        detail_url_template = config.get("detail_url_template")
        if detail_url_template:
            url = detail_url_template.format(post_id=post_id)
        else:
            # í…œí”Œë¦¿ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ íŒ¨í„´ ì‚¬ìš©
            base = config["list_url"].split("/notices/")[0]
            url = f"{base}/notices/{post_id}"
    else:
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ë°©ì‹
        detail_url_template = config.get("detail_url_template")
        if detail_url_template:
            # í…œí”Œë¦¿ì´ ìˆìœ¼ë©´ í…œí”Œë¦¿ ì‚¬ìš©
            url = detail_url_template.format(post_id=post_id)
        else:
            # í…œí”Œë¦¿ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ë°©ì‹
            list_url = config["list_url"]
            id_param = config["id_param"]
            list_params = config.get("list_params", {})

            params = {id_param: post_id}
            params.update(list_params)

            # ìƒëª…ê³¼í•™ê³¼ëŠ” md=v íŒŒë¼ë¯¸í„° ì¶”ê°€
            if dept_key == "DEPT_LIFE_SCIENCE":
                params["md"] = "v"

            # URL ì¡°í•©
            if "?" in list_url:
                url = f"{list_url}&{urlencode(params)}"
            else:
                url = f"{list_url}?{urlencode(params)}"

    headers = {"User-Agent": "Mozilla/5.0"}
    r = requests.get(url, headers=headers, timeout=(10, 20))
    if r.status_code != 200:
        print(f"âŒ {dept_key} ìƒì„¸ ìš”ì²­ ì‹¤íŒ¨ post_id={post_id}, status={r.status_code}")
        return None
    return r.text


def parse_notice_fields_generic(dept_key: str, html: str, post_id: int) -> Optional[dict]:
    """í•™ê³¼ë³„ ë…ë¦½ URL HTML íŒŒì‹± (í†µí•©)"""
    config = DEPT_CONFIGS.get(dept_key)
    if not config:
        return None

    soup = BeautifulSoup(html, "html.parser")
    selectors = config["selectors"]
    department = config["department"]

    # ì œëª© ì¶”ì¶œ
    title = ""
    for sel in selectors.get("title", []):
        if sel == "title":
            # <title> íƒœê·¸ íŠ¹ìˆ˜ ì²˜ë¦¬
            title_el = soup.find("title")
            if title_el:
                title = title_el.get_text(strip=True)
                # "- ì„œìš¸ì‹œë¦½ëŒ€í•™êµ ê²½ì œí•™ë¶€" ê°™ì€ ì ‘ë¯¸ì‚¬ ì œê±°
                title = re.sub(r"\s*[-â€“â€”]\s*ì„œìš¸ì‹œë¦½ëŒ€í•™êµ.*$", "", title).strip()
                break
        else:
            # ì¡°ê²½í•™ê³¼ëŠ” h1 íƒœê·¸ê°€ 2ê°œì´ë¯€ë¡œ ë‘ ë²ˆì§¸ ê²ƒ ì‚¬ìš©
            if dept_key == "DEPT_LANDSCAPE_ARCHITECTURE" and sel == "h1":
                h1_elements = soup.select("h1")
                if len(h1_elements) >= 2:
                    title = h1_elements[1].get_text(" ", strip=True)
                    break
            else:
                title_el = soup.select_one(sel)
                if title_el:
                    title = title_el.get_text(" ", strip=True)
                    break

    # ë³¸ë¬¸ ì¶”ì¶œ
    content_text = ""
    content_selectors = selectors.get("content", [])
    if content_selectors:
        for sel in content_selectors:
            content_el = soup.select_one(sel)
            if content_el:
                content_text = content_el.get_text("\n", strip=True)
                break
    if not content_text:
        content_text = extract_main_text_from_html(html)

    # ë‚ ì§œ/ì¡°íšŒìˆ˜ ì •ë³´ ì¶”ì¶œ
    date_text, view_count = "", 0
    date_info_selectors = selectors.get("date_info", [])

    for sel in date_info_selectors:
        info_el = soup.select_one(sel)
        if info_el:
            text = info_el.get_text(" ", strip=True)

            # ë‚ ì§œ ì¶”ì¶œ
            if not date_text:
                # yyyy-mm-dd í˜•ì‹
                m_date = re.search(r"\d{4}-\d{2}-\d{2}", text)
                if m_date:
                    date_text = m_date.group()
                else:
                    # yyyyë…„ mmì›” ddì¼ í˜•ì‹ (ê²½ì œí•™ë¶€)
                    m_date_kr = re.search(r"(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼", text)
                    if m_date_kr:
                        yyyy = int(m_date_kr.group(1))
                        mm = int(m_date_kr.group(2))
                        dd = int(m_date_kr.group(3))
                        date_text = f"{yyyy:04d}-{mm:02d}-{dd:02d}"
                    else:
                        # "ì›” ì¼, ë…„" í˜•ì‹ (ê±´ì¶•í•™ë¶€: "9ì›” 22, 2025")
                        m_date_mon = re.search(r"(\d{1,2})ì›”\s+(\d{1,2}),\s+(\d{4})", text)
                        if m_date_mon:
                            mm = int(m_date_mon.group(1))
                            dd = int(m_date_mon.group(2))
                            yyyy = int(m_date_mon.group(3))
                            date_text = f"{yyyy:04d}-{mm:02d}-{dd:02d}"
                        else:
                            # yy-mm-dd í˜•ì‹ (í™”í•™ê³µí•™ê³¼)
                            m_date_short = re.search(r'(?<!\d)(?P<yy>\d{2})-(?P<mm>\d{2})-(?P<dd>\d{2})(?!\d)', text)
                            if m_date_short:
                                yy = int(m_date_short['yy'])
                                mm = int(m_date_short['mm'])
                                dd = int(m_date_short['dd'])
                                yyyy = 2000 + yy
                                date_text = f"{yyyy:04d}-{mm:02d}-{dd:02d}"

            # ì¡°íšŒìˆ˜ ì¶”ì¶œ
            if view_count == 0:
                # "ì¡°íšŒìˆ˜ 123" í˜•ì‹
                m_view = re.search(r"ì¡°íšŒìˆ˜\s*([0-9,]+)", text)
                if m_view:
                    view_count = int(m_view.group(1).replace(",", ""))
                else:
                    # "Views 168" í˜•ì‹ (ê±´ì¶•í•™ë¶€)
                    m_view_en = re.search(r"Views?\s+(\d+)", text, re.I)
                    if m_view_en:
                        view_count = int(m_view_en.group(1))

    # í™”í•™ê³µí•™ê³¼ ì¡°íšŒìˆ˜ (ì•„ì´ì½˜ìœ¼ë¡œ ì°¾ê¸°)
    if dept_key == "DEPT_CHEMICAL_ENGINEERING" and view_count == 0:
        view_count_el = soup.select_one("strong > i.fa-eye")
        if view_count_el:
            raw_text = view_count_el.find_previous("strong").text.strip()
            m = re.search(r'\d+', raw_text)
            view_count = int(m.group()) if m else 0

    # ì¡°ê²½í•™ê³¼ íŠ¹ìˆ˜ ì²˜ë¦¬ (div.detail-value ë°°ì—´ì—ì„œ ì¸ë±ìŠ¤ë¡œ íŒŒì‹±)
    if dept_key == "DEPT_LANDSCAPE_ARCHITECTURE":
        detail_values = soup.select("div.detail-value")
        if len(detail_values) >= 3:
            # [1]ë²ˆì§¸: ë‚ ì§œ (2025-10-10 16:17)
            if not date_text:
                date_val = detail_values[1].get_text(strip=True)
                m_date = re.search(r"(\d{4}-\d{2}-\d{2})", date_val)
                if m_date:
                    date_text = m_date.group(1)

            # [2]ë²ˆì§¸: ì¡°íšŒìˆ˜ (19)
            if view_count == 0:
                view_val = detail_values[2].get_text(strip=True)
                if view_val.isdigit():
                    view_count = int(view_val)

    posted_date = date_text or datetime.now().strftime("%Y-%m-%d")

    return {
        "title": title,
        "department": department,
        "posted_date": posted_date,
        "post_number": post_id,
        "content_text": content_text,
        "view_count": view_count,
    }


def process_one_generic(dept_key: str, post_id: int) -> str:
    """í•™ê³¼ë³„ ë…ë¦½ URL ê³µì§€ì‚¬í•­ í•œ ê±´ ì²˜ë¦¬ (í†µí•©)"""
    config = DEPT_CONFIGS.get(dept_key)
    if not config:
        print(f"âŒ ì„¤ì •ë˜ì§€ ì•Šì€ í•™ê³¼: {dept_key}")
        return "skipped_error"

    category = config["category"]
    department = config["department"]
    list_url = config["list_url"]
    id_param = config["id_param"]

    # HTML ê°€ì ¸ì˜¤ê¸°
    html = fetch_notice_html_generic(dept_key, post_id)
    if not html:
        print(f"âš ï¸ {dept_key} {id_param}={post_id}: HTML ë¡œë“œ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
        return "skipped_error"

    # íŒŒì‹±
    parsed = parse_notice_fields_generic(dept_key, html, post_id)
    if not parsed:
        print(f"{dept_key} {id_param}={post_id}: ê²Œì‹œë¬¼ ì—†ìŒ")
        return "not_found"

    post_number = parsed["post_number"]
    title = parsed["title"]
    posted_date = parsed["posted_date"]
    view_count = parsed.get("view_count", 0)

    # ë§í¬ ìƒì„± (url_typeì— ë”°ë¼ ë‹¤ë¦„)
    url_type = config.get("url_type", "query")

    if url_type == "slug":
        # WordPress slug ë°©ì‹: https://uosarch.ac.kr/uosarch_notice/slug-name/
        detail_url_base = config.get("detail_url_base")
        slug = post_id if isinstance(post_id, str) else str(post_id)
        crawl_link = f"{detail_url_base}{slug}/"
        db_link = crawl_link
    elif url_type == "path":
        # ê²½ë¡œ ë°©ì‹: https://econ.uos.ac.kr/notices/19184
        detail_url_template = config.get("detail_url_template")
        if detail_url_template:
            crawl_link = detail_url_template.format(post_id=post_id)
        else:
            base = list_url.split("/notices/")[0]
            crawl_link = f"{base}/notices/{post_id}"
        db_link = crawl_link
    else:
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ë°©ì‹ (í™”í•™ê³µí•™ê³¼, ìƒëª…ê³¼í•™ê³¼, ì¡°ê²½í•™ê³¼)
        detail_url_template = config.get("detail_url_template")
        if detail_url_template:
            # í…œí”Œë¦¿ì´ ìˆìœ¼ë©´ í…œí”Œë¦¿ ì‚¬ìš© (ì¡°ê²½í•™ê³¼)
            crawl_link = detail_url_template.format(post_id=post_id)
        else:
            # í…œí”Œë¦¿ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ë°©ì‹
            list_params = config.get("list_params", {})
            params = {id_param: post_id}
            params.update(list_params)

            if dept_key == "DEPT_LIFE_SCIENCE":
                params["md"] = "v"

            if "?" in list_url:
                crawl_link = f"{list_url}&{urlencode(params)}"
            else:
                crawl_link = f"{list_url}?{urlencode(params)}"

        db_link = crawl_link

    # post_number ì •ê·œí™” (slug -> hash)
    normalized_post_number = _normalize_post_number(post_number)

    # ì¤‘ë³µ ì²´í¬
    prev_dt_raw = get_existing_posted_date(category, post_number)
    prev_dt = _ymd(prev_dt_raw)
    curr_dt = _ymd(posted_date)

    if prev_dt:
        if prev_dt == curr_dt:
            # ë‚ ì§œëŠ” ê°™ì§€ë§Œ ì¡°íšŒìˆ˜ëŠ” ì—…ë°ì´íŠ¸ (ê°€ë²¼ìš´ ì—…ë°ì´íŠ¸)
            print(f"{dept_key} {id_param}={post_id} (post_number={post_number}) ì´ë¯¸ ì¡´ì¬ â†’ ì¡°íšŒìˆ˜ë§Œ ì—…ë°ì´íŠ¸")

            # ì¡°íšŒìˆ˜ë§Œ ì—…ë°ì´íŠ¸
            with mysql_conn() as conn:
                cur = conn.cursor()
                cur.execute(
                    "UPDATE notice SET view_count = %s WHERE category = %s AND post_number = %s",
                    (view_count, category, normalized_post_number)
                )
                cur.close()

            print(f"âœ… ì¡°íšŒìˆ˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ: [{department}] {id_param}={post_id}, viewCount={view_count}")
            return "stored"
        else:
            print(f"{dept_key} {id_param}={post_id} (post_number={post_number}) ë‚ ì§œ ë³€ê²½ {prev_dt} â†’ {curr_dt}, ì—…ë°ì´íŠ¸ ì§„í–‰")

    # HTML ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    html_text = extract_main_text_from_html(html)

    # HTML â†’ ì „ì²´ ì´ë¯¸ì§€ ìº¡ì²˜
    imgs = html_to_images_playwright(
        crawl_link,
        viewport_width=1200,
        slice_height=1800,
        debug_full_image_path=None,
        full_image_format="png",
    )
    if not imgs:
        print(f"â†³ {dept_key} {id_param}={post_id}: ì´ë¯¸ì§€ ìº¡ì²˜ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
        return "skipped_error"

    # í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ë™ì‹œ ìš”ì•½
    summary = summarize_with_text_and_images(html_text, imgs)
    if not summary:
        print(f"â†³ {dept_key} {id_param}={post_id}: í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ìš”ì•½ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
        return "skipped_error"

    print(summary)

    # DB ì—…ì„œíŠ¸
    row = {
        "category": category,
        "post_number": normalized_post_number,  # ì •ê·œí™”ëœ ê°’ ì‚¬ìš©
        "title": title,
        "link": db_link,
        "summary": summary,
        "embedding_vector": None,
        "posted_date": posted_date,
        "department": department,
        "view_count": view_count
    }
    try:
        upsert_notice(row)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: [{department}] {id_param}={post_id}, post_number={normalized_post_number} (ì›ë³¸:{post_number}), title={title}, posted_date={posted_date}, viewCount={view_count}")
        return "stored"
    except MySQLError as e:
        print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e.__class__.__name__}({getattr(e,'errno',None)}): {e}")
        tb = traceback.format_exc(limit=3)
        print(f"â†³ Traceback(ìš”ì•½):\n{tb}")
        return "skipped_error"

# =========================
# 9) ì‹¤í–‰ë¶€
# =========================
def main() -> int:
    print(f"Screenshot directory: {OUT_DIR}")

    # í¬í„¸ ê³µí†µ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬
    portal_targets = [
        # "GENERAL",
        # "ACADEMIC",
        # "COLLEGE_ENGINEERING",
        # "COLLEGE_HUMANITIES",
        # "COLLEGE_SOCIAL_SCIENCES",
        # "COLLEGE_URBAN_SCIENCE",
        # "COLLEGE_ARTS_SPORTS",
        # "COLLEGE_BUSINESS",
        # "COLLEGE_NATURAL_SCIENCES",
        # "COLLEGE_LIBERAL_CONVERGENCE",
    ]

    for cat in portal_targets:
        list_id = CATEGORIES.get(cat)
        if not list_id or "TODO" in list_id.lower():
            print(f"â­ï¸  {cat}: list_id ë¯¸ì„¤ì • â†’ ê±´ë„ˆëœ€")
            continue

        seqs = collect_recent_seqs(list_id, extra_params=None, limit=RECENT_WINDOW, max_pages=10)

        if not seqs:
            print(f"âš ï¸ {cat}: ëª©ë¡ì—ì„œ seqë¥¼ ì°¾ì§€ ëª»í•´ ê±´ë„ˆëœ€")
            continue

        print(f"==== [{cat}] list_id={list_id}, {len(seqs)}ê°œ ìˆ˜ì§‘ë¨ (ëª©ë¡ ë…¸ì¶œ í•­ëª©ë§Œ) ====")
        for seq in reversed(seqs):
            process_one(cat, list_id, seq)
            time.sleep(REQUEST_SLEEP)

    # í•™ê³¼ë³„ ë…ë¦½ URL ì²˜ë¦¬ (í†µí•© ë°©ì‹)
    for dept_key in DEPT_CONFIGS.keys():
        config = DEPT_CONFIGS[dept_key]
        department = config["department"]

        #í•´ë‹¹ ë¶€ë¶„ì´ í•™ê³¼ë³„ ë…ë¦½ ë§í¬ì—ì„œ ê°ê° ëª‡ê°œì”© ê°€ì ¸ì˜¬ì§€ë¥¼ ì„¤ì •
        seqs = collect_recent_seqs_generic(dept_key, limit=1)

        if not seqs:
            print(f"âš ï¸ [{department}]: ëª©ë¡ì—ì„œ ê²Œì‹œë¬¼ì„ ì°¾ì§€ ëª»í•´ ê±´ë„ˆëœ€")
            continue

        print(f"==== [{department}] {len(seqs)}ê°œ ìˆ˜ì§‘ë¨ ====", flush=True)
        for post_id in reversed(seqs):
            process_one_generic(dept_key, post_id)
            time.sleep(REQUEST_SLEEP)

    return 0

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        log(f"ERROR: {type(e).__name__}: {e}")
        traceback.print_exc()
        sys.exit(1)